{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0aZBF7bM2HPqeZn26dcs8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reshape-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamad-Atif1/paper2code/blob/main/Transformers/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is made by **Eng. Mohammed Alshabrawi**"
      ],
      "metadata": {
        "id": "DmvDdmcfa58S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention is all you need**\n",
        "\n",
        "Transformers are the dominant neural network architecture for sequential datasets due to their ability to model long-range dependencies and process sequences in parallel.\n",
        "\n",
        "To gain a deep understanding of how they work, I built a Transformer model from scratch, implementing core components such as multi-head self-attention, positional encoding, and cross-attention.\n",
        "\n"
      ],
      "metadata": {
        "id": "11nF0c8YZ--a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://miro.medium.com/v2/resize:fit:676/1*MU9no9JcYWJCeDE7zc5vsQ.png\" height=500>\n",
        "</div>"
      ],
      "metadata": {
        "id": "87vFiZd_iXaA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dBcltR4cu1-o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5IhEDkck-8A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding"
      ],
      "metadata": {
        "id": "AkF25c08-9MS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us start with **Positional Encoding**\n",
        "\n",
        "Transformers don't have a built-in sense of order (unlike RNNs), so positional encodings provide a way for the model to understand the relative and absolute position of tokens by adding sinusoidal vectors to the input tokens .\n",
        "\n",
        "Alternatively, this can be seen as each input token being conditioned on its corresponding positional encoding vector. Positional Encoding fourmla:\n",
        "\n",
        "$$ PE_(pos,2i) = sin(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}) $$\n",
        "\n",
        "$$ PE_(pos,2i) = cos(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}) $$\n",
        "\n",
        "for efficiency and numerical stability, we will use exp and ln to implement the denominator.\n",
        "\n",
        "\n",
        "$$ {10000^{\\frac{-2i}{d_{\\text{model}}}}} = e^{ \\ln(10000)^{\\frac{-2i}{d_{\\text{model}}}}} $$\n",
        "$$ {10000^{\\frac{-2i}{d_{\\text{model}}}}} = e^{ \\frac{-2i}{{d_{\\text{model}}}}  \\ln(10000)} $$\n"
      ],
      "metadata": {
        "id": "x5FqNcuhl6O8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, embed_size)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # add max_len dim\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # add bs dim , shape: (1, max_len, embed_size)\n",
        "        self.register_buffer('pe', pe) # This save pe as non trainable parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len (num of tokens), embed_size)\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:, :seq_len]\n"
      ],
      "metadata": {
        "id": "tId-z2S3SBJX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head attention (MHA)"
      ],
      "metadata": {
        "id": "zZcbTi3x8eK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main component in Transformers is the attention layer. There are many varieties of attention layers, such as Multi-head attention (MHA), Grouped-Query Attention (GQA), and Multi-Query Attention (MQA). These versions are almost similar with fewer parameters.\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r-3sWaUT4K-5ogX99hqT0A.png\" height=200>\n",
        "\n",
        "The core idea of an attention layer focuses on around three learnable matrices: **Query (Q)**, **Key (K)**, and **Value (V)**, all derived from the input embeddings. Conceptually, a Query seeks out relevant Keys. When a Query vector is multiplied with all Key vectors (typically using a dot product to measure similartiy), it generates attention scores. These scores indicate how related each Key is to the Query (how each token to another token) .\n",
        "\n",
        "These raw attention scores are normalized through a softmax function, transforming them into a probability distribution. This normalized distribution determines the \"attention weights\" or enegry.  Finally, these attention weights are used to compute a weighted sum of the Value vectors.\n",
        "\n",
        "**Multi-Head Attention (MHA)** is all about splitting the input embedding into N number of heads that are then independently processed in parallel."
      ],
      "metadata": {
        "id": "T357nHzy_IWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,embed_size,heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "\n",
        "        assert (self.head_dim * heads == embed_size), \"embed_size // heads is not divisible\"\n",
        "\n",
        "        self.queries = nn.Linear(self.embed_size,self.embed_size)\n",
        "        self.keys = nn.Linear(self.embed_size,self.embed_size)\n",
        "        self.values = nn.Linear(self.embed_size,self.embed_size)\n",
        "\n",
        "        self.fc = nn.Linear(self.embed_size, self.embed_size)\n",
        "\n",
        "    def forward(self,q,k,v,mask):\n",
        "        q = self.queries(q)\n",
        "        k = self.keys(k)\n",
        "        v = self.values(v)\n",
        "        # split the embedding into k heads\n",
        "        # q,v,k shapes: [bs, num_of_tokens (N) , heads * head_dim ]\n",
        "        # Now techincally, we will work on each head independently, as if there were batches of heads!\n",
        "        # (batch_size * heads, num_tokens, head_dim)\n",
        "        q = q.reshape(q.shape[0]*self.heads, q.shape[1], self.head_dim)\n",
        "        k = k.reshape(k.shape[0]*self.heads, k.shape[1], self.head_dim)\n",
        "        v = v.reshape(v.shape[0]*self.heads, v.shape[1], self.head_dim)\n",
        "\n",
        "        energy = torch.bmm(q,k.permute(0,2,1)) # Q*K^T -> bs, q_N, k_N\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask==0,float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy/(self.head_dim ** (0.5)), dim=2)\n",
        "        # \n",
        "\n",
        "        out = torch.bmm(attention,v)\n",
        "        out = out.reshape(q.shape[0]//self.heads, q.shape[1], self.embed_size) # bs,N,h*heads\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JeaGWyXwv1r9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "s-FYXyTLGAOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each layer within the Encoder  consists of two key sub-layers: a Multi-Head Attention mechanism (as described above, allowing each token to attend to all other tokens in the sequence) and a simple feed-forward neural network. After each sub-layer, Layer normlization and resdual connection are applied as shown in the fig\n",
        "\n",
        "Encoders are mainly used to process the input tokens\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\" width=200 >"
      ],
      "metadata": {
        "id": "cXC2_9dvHG-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,embed_size,heads, ff_expantion,dout=0.1):\n",
        "        super(EncoderBlock,self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_size,heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size,embed_size*ff_expantion),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_size*ff_expantion,embed_size))\n",
        "        self.dout = nn.Dropout(dout)\n",
        "\n",
        "\n",
        "    def forward(self,q,k,v,mask):\n",
        "        sub_layer_one = self.attention(q,k,v,mask)\n",
        "        sub_layer_one = self.dout(sub_layer_one)\n",
        "        sub_layer_one += q # skip connection\n",
        "        sub_layer_one = self.norm1(sub_layer_one)\n",
        "\n",
        "        sub_layer_two = self.feed_forward(sub_layer_one)\n",
        "        sub_layer_two = self.dout(sub_layer_two)\n",
        "        sub_layer_two += sub_layer_one\n",
        "        sub_layer_two = self.norm2(sub_layer_two)\n",
        "\n",
        "\n",
        "\n",
        "        return sub_layer_two\n"
      ],
      "metadata": {
        "id": "EL4RoIGK0W6g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self,vocab_size,embed_size,heads,ff_expansion,num_layers=6, dout=0.1,max_len=5000):\n",
        "\n",
        "        super(Encoder,self).__init__()\n",
        "        self.word_embedding = nn.Embedding(vocab_size,embed_size)\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.pe = PositionalEncoding(embed_size,max_len)\n",
        "        self.dout = nn.Dropout(dout)\n",
        "        for i in range(num_layers):\n",
        "            self.layers.append(EncoderBlock(embed_size,heads, ff_expansion,dout=0.1))\n",
        "\n",
        "\n",
        "    def forward(self,x,mask):\n",
        "        x = self.word_embedding(x)\n",
        "        x = self.dout(self.pe(x))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x,x,x,mask)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "XjYmPBUddO7t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "kojiBqZ4GCYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Decoder in a Transformer shares many similarities with the Encoder, also comprising a stack of identical layers. However, it is designed for generating output sequences, so it can onlt attending to the previous tokens.  We use casual mask to prevent the decoder from payinh attention to the next token. casual mask is simply a lower triangler matrix.\n",
        "\n",
        "The Decoder consists of two attention layers and a feed-forward layer. The first one is for paying attention to the output embedding or the previously generated tokens (self-attention). The next one pays attention to the output of the Encoder layer.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:676/1*MU9no9JcYWJCeDE7zc5vsQ.png\" height=400>"
      ],
      "metadata": {
        "id": "HUMF3OOiKMJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, ff_expansion, dout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.attention = MultiHeadAttention(embed_size, heads=heads)\n",
        "        self.encoder_block = EncoderBlock(\n",
        "            embed_size, heads, ff_expansion,dout\n",
        "        )\n",
        "        self.dout = nn.Dropout(dout)\n",
        "\n",
        "    def forward(self, x, value, key, cross_mask, trg_mask):\n",
        "        self_attention = self.attention(x, x, x, trg_mask)\n",
        "        self_attention = self.dout(self_attention)\n",
        "        self_attention += x\n",
        "        self_attention = self.norm(self_attention)\n",
        "        # the output of the self attention layer = query to the cross attention\n",
        "        # Cross_attention:\n",
        "        #value,key (encoder) and 'query' (decoder) are fed into the encoder block.\n",
        "        out = self.encoder_block(self_attention,key,value, cross_mask)\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IZ5O_ZUWzWpQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        ff_expansion,\n",
        "        dout,\n",
        "        max_len,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.pe = PositionalEncoding(embed_size,max_len)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_size, heads, ff_expansion, dout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dout = nn.Dropout(dout)\n",
        "\n",
        "    def forward(self, x, enc_out, cross_mask, trg_mask):\n",
        "        x = self.word_embedding(x)\n",
        "        x = self.dout(self.pe(x))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, enc_out, cross_mask, trg_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "bJh4SPWL2R-0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting all together"
      ],
      "metadata": {
        "id": "UVvJD9u_GGP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        trg_pad_idx,\n",
        "        forward_expansion=4,\n",
        "        embed_size=512,\n",
        "        num_layers=6,\n",
        "        ff_expansion=4,\n",
        "        heads=8,\n",
        "        dout=0.1,\n",
        "        max_len=5000,\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.heads = heads\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size,\n",
        "            embed_size,\n",
        "            heads,\n",
        "            ff_expansion,\n",
        "            num_layers,\n",
        "            dout,\n",
        "            max_len\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            trg_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            ff_expansion,\n",
        "            dout,\n",
        "            max_len,\n",
        "        )\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        N, src_len = src.shape\n",
        "\n",
        "        pad_mask = (src != self.src_pad_idx)\n",
        "\n",
        "        # expand to (N, src_len, src_len) to match self-attention mask shapes\n",
        "        src_mask = pad_mask.unsqueeze(1).expand(N, src_len, src_len)\n",
        "\n",
        "        # repeat for all heads: (N * heads, src_len, src_len) to match self-attention mask shapes\n",
        "        src_mask = src_mask.repeat(self.heads, 1, 1)\n",
        "\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "\n",
        "        pad_mask = (trg != self.trg_pad_idx)\n",
        "\n",
        "        causal_mask = torch.tril(torch.ones(trg_len, trg_len)).bool()\n",
        "\n",
        "        # repeat for all heads: (N * heads, trg_len, trg_len) to match self-attention mask shapes\n",
        "        trg_mask = causal_mask.repeat(self.heads*N, 1, 1)\n",
        "\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def make_cross_mask(self, src, trg):\n",
        "        \"\"\"(decoder attending to encoder)\"\"\"\n",
        "        N, src_len = src.shape\n",
        "        _, trg_len = trg.shape\n",
        "\n",
        "        src_pad_mask = (src != self.src_pad_idx)\n",
        "\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx)\n",
        "\n",
        "        # cross-attention mask: (N, trg_len, src_len)\n",
        "        cross_mask = trg_pad_mask.unsqueeze(2) & src_pad_mask.unsqueeze(1)\n",
        "\n",
        "        # repeat for all heads: (N * heads, trg_len, src_len) to match self-attention mask shapes\n",
        "        cross_mask = cross_mask.repeat(self.heads, 1, 1)\n",
        "\n",
        "        return cross_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # Create masks\n",
        "        src_mask = self.make_src_mask(src)           # For encoder self-attention\n",
        "        trg_mask = self.make_trg_mask(trg)           # For decoder self-attention\n",
        "        cross_mask = self.make_cross_mask(src, trg)  # For decoder cross-attention\n",
        "\n",
        "        # Forward pass\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "        # Pass cross_mask to decoder instead of src_mask\n",
        "        out = self.decoder(trg, enc_src, cross_mask, trg_mask)\n",
        "        return out"
      ],
      "metadata": {
        "id": "eVrALDup2WjU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test code\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
        "    device\n",
        ")\n",
        "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
        "\n",
        "src_pad_idx = 0\n",
        "trg_pad_idx = 0\n",
        "src_vocab_size = 80\n",
        "trg_vocab_size = 80\n",
        "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(\n",
        "    device\n",
        ")\n",
        "out = model(x, trg)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "5Ga_CPZr2Zk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd83e0c0-3369-41f5-e041-a439ea63ee97"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "torch.Size([2, 8, 80])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let us test it on a Machine Translation task!"
      ],
      "metadata": {
        "id": "5qFOetftnNzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import pickle\n",
        "\n"
      ],
      "metadata": {
        "id": "iOukK37sNAjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
        "        self.idx2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
        "        self.word_count = Counter()\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split():\n",
        "            self.word_count[word] += 1\n",
        "            if word not in self.word2idx:\n",
        "                idx = len(self.word2idx)\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "\n",
        "    def build_vocab(self, sentences, min_freq=1):\n",
        "        for sentence in sentences:\n",
        "            for word in sentence.split():\n",
        "                self.word_count[word] += 1\n",
        "\n",
        "        for word, count in self.word_count.items():\n",
        "            if count >= min_freq and word not in self.word2idx:\n",
        "                idx = len(self.word2idx)\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "\n",
        "    def sentence_to_indices(self, sentence):\n",
        "        return [self.word2idx.get(word, self.word2idx[\"<unk>\"]) for word in sentence.split()]\n",
        "\n",
        "    def indices_to_sentence(self, indices):\n",
        "        return \" \".join([self.idx2word[idx] for idx in indices if idx not in [0, 1, 2]])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ],
      "metadata": {
        "id": "4gWBhbK0M_qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len=100):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_sentence = self.src_sentences[idx]\n",
        "        tgt_sentence = self.tgt_sentences[idx]\n",
        "\n",
        "        src_indices = [self.src_vocab.word2idx[\"<sos>\"]] + \\\n",
        "                     self.src_vocab.sentence_to_indices(src_sentence) + \\\n",
        "                     [self.src_vocab.word2idx[\"<eos>\"]]\n",
        "\n",
        "        tgt_indices = [self.tgt_vocab.word2idx[\"<sos>\"]] + \\\n",
        "                     self.tgt_vocab.sentence_to_indices(tgt_sentence) + \\\n",
        "                     [self.tgt_vocab.word2idx[\"<eos>\"]]\n",
        "\n",
        "        src_indices = src_indices[:self.max_len]\n",
        "        tgt_indices = tgt_indices[:self.max_len]\n",
        "\n",
        "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
        "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
        "    return src_batch, tgt_batch"
      ],
      "metadata": {
        "id": "CwOpeyM9NGUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_translation_data():\n",
        "    \"\"\"Create 100 English-Arabic translation pairs for training + test data\"\"\"\n",
        "\n",
        "    # Training data - 100 pairs\n",
        "    en_train = [\n",
        "        # Greetings and basic conversation\n",
        "        \"hello how are you today\",\n",
        "        \"good morning my friend\",\n",
        "        \"good evening everyone\",\n",
        "        \"have a nice day\",\n",
        "        \"see you later\",\n",
        "        \"goodbye and take care\",\n",
        "        \"nice to meet you\",\n",
        "        \"how have you been\",\n",
        "        \"i hope you are well\",\n",
        "        \"thank you very much\",\n",
        "\n",
        "        # Personal information\n",
        "        \"what is your name\",\n",
        "        \"my name is ahmed\",\n",
        "        \"my name is sara\",\n",
        "        \"my name is mohammed\",\n",
        "        \"where are you from\",\n",
        "        \"i am from egypt\",\n",
        "        \"i am from saudi arabia\",\n",
        "        \"i am from jordan\",\n",
        "        \"i am from lebanon\",\n",
        "        \"i live in cairo\",\n",
        "\n",
        "        # Age and family\n",
        "        \"how old are you\",\n",
        "        \"i am twenty years old\",\n",
        "        \"i am thirty years old\",\n",
        "        \"i am a young man\",\n",
        "        \"i am a young woman\",\n",
        "        \"do you have siblings\",\n",
        "        \"i have two brothers\",\n",
        "        \"i have one sister\",\n",
        "        \"i have a big family\",\n",
        "        \"my family is small\",\n",
        "\n",
        "        # Occupation and education\n",
        "        \"what do you do\",\n",
        "        \"i am a student\",\n",
        "        \"i am a teacher\",\n",
        "        \"i am a doctor\",\n",
        "        \"i am an engineer\",\n",
        "        \"where do you work\",\n",
        "        \"i work in a hospital\",\n",
        "        \"i work in a school\",\n",
        "        \"i study at university\",\n",
        "        \"i study medicine\",\n",
        "\n",
        "        # Hobbies and interests\n",
        "        \"what do you like\",\n",
        "        \"i like reading books\",\n",
        "        \"i like watching movies\",\n",
        "        \"i love playing football\",\n",
        "        \"i enjoy listening to music\",\n",
        "        \"do you like sports\",\n",
        "        \"yes i love sports\",\n",
        "        \"i play basketball\",\n",
        "        \"i like swimming\",\n",
        "        \"i enjoy cooking\",\n",
        "\n",
        "        # Weather and time\n",
        "        \"how is the weather\",\n",
        "        \"the weather is nice\",\n",
        "        \"it is sunny today\",\n",
        "        \"it is raining outside\",\n",
        "        \"it is very hot\",\n",
        "        \"it is cold today\",\n",
        "        \"what time is it\",\n",
        "        \"it is three o clock\",\n",
        "        \"it is morning time\",\n",
        "        \"it is evening now\",\n",
        "\n",
        "        # Food and drink\n",
        "        \"are you hungry\",\n",
        "        \"yes i am hungry\",\n",
        "        \"what do you want to eat\",\n",
        "        \"i want to eat rice\",\n",
        "        \"i like arabic food\",\n",
        "        \"do you want some tea\",\n",
        "        \"yes please give me tea\",\n",
        "        \"i prefer coffee\",\n",
        "        \"the food is delicious\",\n",
        "        \"i am thirsty\",\n",
        "\n",
        "        # Places and directions\n",
        "        \"where is the mosque\",\n",
        "        \"the mosque is near here\",\n",
        "        \"where is the hospital\",\n",
        "        \"go straight then turn right\",\n",
        "        \"the school is far\",\n",
        "        \"the market is close\",\n",
        "        \"i want to go home\",\n",
        "        \"where do you live\",\n",
        "        \"i live in this city\",\n",
        "        \"the house is big\",\n",
        "\n",
        "        # Daily activities\n",
        "        \"what are you doing\",\n",
        "        \"i am reading a book\",\n",
        "        \"i am watching television\",\n",
        "        \"i am going to sleep\",\n",
        "        \"i wake up early\",\n",
        "        \"i go to work\",\n",
        "        \"i study every day\",\n",
        "        \"i help my mother\",\n",
        "        \"i play with friends\",\n",
        "        \"i eat breakfast\",\n",
        "\n",
        "        # Shopping and money\n",
        "        \"how much does this cost\",\n",
        "        \"this costs ten dollars\",\n",
        "        \"it is very expensive\",\n",
        "        \"it is cheap\",\n",
        "        \"where can i buy this\",\n",
        "        \"you can buy it here\",\n",
        "        \"i need to go shopping\",\n",
        "        \"do you have money\",\n",
        "        \"i have some money\",\n",
        "        \"i want to buy clothes\",\n",
        "\n",
        "        # Travel and transportation\n",
        "        \"i want to travel\",\n",
        "        \"where do you want to go\",\n",
        "        \"i want to visit mecca\",\n",
        "        \"how do you go to work\",\n",
        "        \"i go by car\",\n",
        "        \"i take the bus\",\n",
        "        \"the plane is fast\",\n",
        "        \"the trip was long\",\n",
        "        \"i love traveling\",\n",
        "        \"when will you return\"\n",
        "    ]\n",
        "\n",
        "    ar_train = [\n",
        "        # Greetings and basic conversation\n",
        "        \"مرحباً كيف حالك اليوم\",\n",
        "        \"صباح الخير يا صديقي\",\n",
        "        \"مساء الخير للجميع\",\n",
        "        \"أتمنى لك يوماً سعيداً\",\n",
        "        \"أراك لاحقاً\",\n",
        "        \"وداعاً واعتن بنفسك\",\n",
        "        \"سررت بلقائك\",\n",
        "        \"كيف كان حالك\",\n",
        "        \"أتمنى أن تكون بخير\",\n",
        "        \"شكراً لك جزيلاً\",\n",
        "\n",
        "        # Personal information\n",
        "        \"ما اسمك\",\n",
        "        \"اسمي أحمد\",\n",
        "        \"اسمي سارة\",\n",
        "        \"اسمي محمد\",\n",
        "        \"من أين أنت\",\n",
        "        \"أنا من مصر\",\n",
        "        \"أنا من السعودية\",\n",
        "        \"أنا من الأردن\",\n",
        "        \"أنا من لبنان\",\n",
        "        \"أعيش في القاهرة\",\n",
        "\n",
        "        # Age and family\n",
        "        \"كم عمرك\",\n",
        "        \"عمري عشرون سنة\",\n",
        "        \"عمري ثلاثون سنة\",\n",
        "        \"أنا شاب\",\n",
        "        \"أنا شابة\",\n",
        "        \"هل لديك إخوة\",\n",
        "        \"لدي أخوان\",\n",
        "        \"لدي أخت واحدة\",\n",
        "        \"لدي عائلة كبيرة\",\n",
        "        \"عائلتي صغيرة\",\n",
        "\n",
        "        # Occupation and education\n",
        "        \"ماذا تعمل\",\n",
        "        \"أنا طالب\",\n",
        "        \"أنا مدرس\",\n",
        "        \"أنا طبيب\",\n",
        "        \"أنا مهندس\",\n",
        "        \"أين تعمل\",\n",
        "        \"أعمل في مستشفى\",\n",
        "        \"أعمل في مدرسة\",\n",
        "        \"أدرس في الجامعة\",\n",
        "        \"أدرس الطب\",\n",
        "\n",
        "        # Hobbies and interests\n",
        "        \"ماذا تحب\",\n",
        "        \"أحب قراءة الكتب\",\n",
        "        \"أحب مشاهدة الأفلام\",\n",
        "        \"أحب لعب كرة القدم\",\n",
        "        \"أستمتع بسماع الموسيقى\",\n",
        "        \"هل تحب الرياضة\",\n",
        "        \"نعم أحب الرياضة\",\n",
        "        \"ألعب كرة السلة\",\n",
        "        \"أحب السباحة\",\n",
        "        \"أستمتع بالطبخ\",\n",
        "\n",
        "        # Weather and time\n",
        "        \"كيف الطقس\",\n",
        "        \"الطقس جميل\",\n",
        "        \"الجو مشمس اليوم\",\n",
        "        \"الجو ممطر بالخارج\",\n",
        "        \"الجو حار جداً\",\n",
        "        \"الجو بارد اليوم\",\n",
        "        \"كم الساعة\",\n",
        "        \"الساعة الثالثة\",\n",
        "        \"الوقت صباح\",\n",
        "        \"الوقت مساء الآن\",\n",
        "\n",
        "        # Food and drink\n",
        "        \"هل أنت جائع\",\n",
        "        \"نعم أنا جائع\",\n",
        "        \"ماذا تريد أن تأكل\",\n",
        "        \"أريد أن آكل رز\",\n",
        "        \"أحب الطعام العربي\",\n",
        "        \"هل تريد بعض الشاي\",\n",
        "        \"نعم من فضلك أعطني شاي\",\n",
        "        \"أفضل القهوة\",\n",
        "        \"الطعام لذيذ\",\n",
        "        \"أنا عطشان\",\n",
        "\n",
        "        # Places and directions\n",
        "        \"أين المسجد\",\n",
        "        \"المسجد قريب من هنا\",\n",
        "        \"أين المستشفى\",\n",
        "        \"اذهب مستقيماً ثم انعطف يميناً\",\n",
        "        \"المدرسة بعيدة\",\n",
        "        \"السوق قريب\",\n",
        "        \"أريد أن أذهب للبيت\",\n",
        "        \"أين تسكن\",\n",
        "        \"أسكن في هذه المدينة\",\n",
        "        \"البيت كبير\",\n",
        "\n",
        "        # Daily activities\n",
        "        \"ماذا تفعل\",\n",
        "        \"أقرأ كتاباً\",\n",
        "        \"أشاهد التلفزيون\",\n",
        "        \"سأذهب للنوم\",\n",
        "        \"أستيقظ مبكراً\",\n",
        "        \"أذهب للعمل\",\n",
        "        \"أدرس كل يوم\",\n",
        "        \"أساعد أمي\",\n",
        "        \"ألعب مع الأصدقاء\",\n",
        "        \"آكل الفطور\",\n",
        "\n",
        "        # Shopping and money\n",
        "        \"كم يكلف هذا\",\n",
        "        \"هذا يكلف عشرة دولارات\",\n",
        "        \"إنه غالي جداً\",\n",
        "        \"إنه رخيص\",\n",
        "        \"أين يمكنني شراء هذا\",\n",
        "        \"يمكنك شراؤه هنا\",\n",
        "        \"أحتاج للذهاب للتسوق\",\n",
        "        \"هل لديك مال\",\n",
        "        \"لدي بعض المال\",\n",
        "        \"أريد شراء ملابس\",\n",
        "\n",
        "        # Travel and transportation\n",
        "        \"أريد أن أسافر\",\n",
        "        \"أين تريد أن تذهب\",\n",
        "        \"أريد زيارة مكة\",\n",
        "        \"كيف تذهب للعمل\",\n",
        "        \"أذهب بالسيارة\",\n",
        "        \"آخذ الحافلة\",\n",
        "        \"الطائرة سريعة\",\n",
        "        \"الرحلة كانت طويلة\",\n",
        "        \"أحب السفر\",\n",
        "        \"متى ستعود\"\n",
        "    ]\n",
        "\n",
        "    # Test data - 20 pairs\n",
        "    en_test = [\n",
        "        \"hello my name is omar\",\n",
        "        \"i am a student at university\",\n",
        "        \"the weather is beautiful today\",\n",
        "        \"i want to drink some water\",\n",
        "        \"where is the nearest restaurant\",\n",
        "        \"i like to read books in arabic\",\n",
        "        \"how much does this book cost\",\n",
        "        \"i am going to visit my family\",\n",
        "        \"do you speak english well\",\n",
        "        \"what time does the store open\",\n",
        "        \"i need to buy some groceries\",\n",
        "        \"the city is very crowded\",\n",
        "        \"i work as a software engineer\",\n",
        "        \"can you help me please\",\n",
        "        \"i am learning arabic language\",\n",
        "        \"the hospital is on the main street\",\n",
        "        \"i have been living here for two years\",\n",
        "        \"what is your favorite food\",\n",
        "        \"i want to travel to dubai\",\n",
        "        \"thank you for your help\"\n",
        "    ]\n",
        "\n",
        "    ar_test = [\n",
        "        \"مرحباً اسمي عمر\",\n",
        "        \"أنا طالب في الجامعة\",\n",
        "        \"الطقس جميل اليوم\",\n",
        "        \"أريد أن أشرب بعض الماء\",\n",
        "        \"أين أقرب مطعم\",\n",
        "        \"أحب قراءة الكتب بالعربية\",\n",
        "        \"كم يكلف هذا الكتاب\",\n",
        "        \"سأذهب لزيارة عائلتي\",\n",
        "        \"هل تتكلم الإنجليزية جيداً\",\n",
        "        \"متى يفتح المتجر\",\n",
        "        \"أحتاج لشراء بعض البقالة\",\n",
        "        \"المدينة مزدحمة جداً\",\n",
        "        \"أعمل كمهندس برمجيات\",\n",
        "        \"هل يمكنك مساعدتي من فضلك\",\n",
        "        \"أتعلم اللغة العربية\",\n",
        "        \"المستشفى في الشارع الرئيسي\",\n",
        "        \"أعيش هنا منذ سنتين\",\n",
        "        \"ما طعامك المفضل\",\n",
        "        \"أريد السفر إلى دبي\",\n",
        "        \"شكراً لك على مساعدتك\"\n",
        "    ]\n",
        "\n",
        "    return en_train, ar_train, en_test, ar_test"
      ],
      "metadata": {
        "id": "ucptUcIiNeWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, src_vocab, tgt_vocab, device):\n",
        "    \"\"\"Evaluate model on test data\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_samples = 0\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in test_loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "\n",
        "            output = output.reshape(-1, output.size(-1))\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt_output)\n",
        "            total_loss += loss.item()\n",
        "            num_samples += 1\n",
        "\n",
        "    avg_loss = total_loss / num_samples\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return avg_loss, perplexity\n",
        "\n",
        "def translate_sentence(model, sentence, src_vocab, tgt_vocab, device, max_len=50):\n",
        "    \"\"\"Translate a single sentence\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        src_indices = [src_vocab.word2idx[\"<sos>\"]] + \\\n",
        "                     src_vocab.sentence_to_indices(sentence) + \\\n",
        "                     [src_vocab.word2idx[\"<eos>\"]]\n",
        "        src_tensor = torch.tensor(src_indices).unsqueeze(0).to(device)\n",
        "\n",
        "        tgt_indices = [tgt_vocab.word2idx[\"<sos>\"]]\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            next_token = output[0, -1, :].argmax().item()\n",
        "            tgt_indices.append(next_token)\n",
        "\n",
        "            if next_token == tgt_vocab.word2idx[\"<eos>\"]:\n",
        "                break\n",
        "\n",
        "        translated = tgt_vocab.indices_to_sentence(tgt_indices[1:])\n",
        "        return translated\n",
        "\n",
        "def train_model_with_validation(model, train_loader, test_loader, optimizer, criterion,\n",
        "                              src_vocab, tgt_vocab, device, num_epochs=50):\n",
        "    \"\"\"Train model with validation\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt_input)\n",
        "\n",
        "            output = output.reshape(-1, output.size(-1))\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt_output)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / num_batches\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        test_loss, perplexity = evaluate_model(model, test_loader, src_vocab, tgt_vocab, device)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'  Train Loss: {avg_train_loss:.4f}')\n",
        "        print(f'  Test Loss: {test_loss:.4f}')\n",
        "        print(f'  Perplexity: {perplexity:.2f}')\n",
        "\n",
        "        # test some translations every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(\"\\nSample translations:\")\n",
        "            test_sentences = [\"hello how are you\", \"i am a student\", \"what is your name\"]\n",
        "            for sentence in test_sentences:\n",
        "                translation = translate_sentence(model, sentence, src_vocab, tgt_vocab, device)\n",
        "                print(f\"EN: {sentence} -> AR: {translation}\")\n",
        "            print()\n",
        "\n",
        "    return train_losses, test_losses\n"
      ],
      "metadata": {
        "id": "1XiYV4M4NnDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "en_train, ar_train, en_test, ar_test = create_translation_data()\n",
        "\n",
        "print(f\"training samples: {len(en_train)}\")\n",
        "print(f\"test samples: {len(en_test)}\")\n",
        "\n",
        "# Build vocabularies on training data only\n",
        "src_vocab = Vocabulary()\n",
        "tgt_vocab = Vocabulary()\n",
        "\n",
        "src_vocab.build_vocab(en_train, min_freq=1)\n",
        "tgt_vocab.build_vocab(ar_train, min_freq=1)\n",
        "\n",
        "print(f\"source vocabulary size: {len(src_vocab)}\")\n",
        "print(f\"target vocabulary size: {len(tgt_vocab)}\")\n",
        "\n",
        "train_dataset = TranslationDataset(en_train, ar_train, src_vocab, tgt_vocab)\n",
        "test_dataset = TranslationDataset(en_test, ar_test, src_vocab, tgt_vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "src_pad_idx = 0\n",
        "trg_pad_idx = 0\n",
        "\n",
        "model = Transformer(len(src_vocab), len(tgt_vocab), src_pad_idx, trg_pad_idx).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "print(\"Ready to train with expanded dataset!\")\n",
        "\n",
        "train_losses, test_losses = train_model_with_validation(\n",
        "    model, train_loader, test_loader, optimizer, criterion,\n",
        "    src_vocab, tgt_vocab, device, num_epochs=50\n",
        ")"
      ],
      "metadata": {
        "id": "3bskaOdzoDXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "668c16dc-9d30-4d34-e3a7-542f052a8d0f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training samples: 110\n",
            "Test samples: 20\n",
            "Source vocabulary size: 182\n",
            "Target vocabulary size: 196\n",
            "Ready to train with expanded dataset!\n",
            "Epoch 1/50:\n",
            "  Train Loss: 4.3911\n",
            "  Test Loss: 4.9349\n",
            "  Perplexity: 139.06\n",
            "Epoch 2/50:\n",
            "  Train Loss: 3.2054\n",
            "  Test Loss: 4.6247\n",
            "  Perplexity: 101.97\n",
            "Epoch 3/50:\n",
            "  Train Loss: 2.4582\n",
            "  Test Loss: 4.6906\n",
            "  Perplexity: 108.92\n",
            "Epoch 4/50:\n",
            "  Train Loss: 1.9074\n",
            "  Test Loss: 4.6410\n",
            "  Perplexity: 103.65\n",
            "Epoch 5/50:\n",
            "  Train Loss: 1.5615\n",
            "  Test Loss: 4.8004\n",
            "  Perplexity: 121.56\n",
            "Epoch 6/50:\n",
            "  Train Loss: 1.3368\n",
            "  Test Loss: 4.9773\n",
            "  Perplexity: 145.08\n",
            "Epoch 7/50:\n",
            "  Train Loss: 1.1771\n",
            "  Test Loss: 4.7552\n",
            "  Perplexity: 116.19\n",
            "Epoch 8/50:\n",
            "  Train Loss: 1.1373\n",
            "  Test Loss: 4.9644\n",
            "  Perplexity: 143.22\n",
            "Epoch 9/50:\n",
            "  Train Loss: 0.9428\n",
            "  Test Loss: 4.9482\n",
            "  Perplexity: 140.92\n",
            "Epoch 10/50:\n",
            "  Train Loss: 0.9236\n",
            "  Test Loss: 5.0070\n",
            "  Perplexity: 149.45\n",
            "\n",
            "Sample translations:\n",
            "EN: hello how are you -> AR: يمكنك شراؤه هنا\n",
            "EN: i am a student -> AR: سررت\n",
            "EN: what is your name -> AR: سررت بلقائك\n",
            "\n",
            "Epoch 11/50:\n",
            "  Train Loss: 0.8797\n",
            "  Test Loss: 5.1469\n",
            "  Perplexity: 171.89\n",
            "Epoch 12/50:\n",
            "  Train Loss: 0.8263\n",
            "  Test Loss: 4.9681\n",
            "  Perplexity: 143.76\n",
            "Epoch 13/50:\n",
            "  Train Loss: 0.7879\n",
            "  Test Loss: 4.8321\n",
            "  Perplexity: 125.48\n",
            "Epoch 14/50:\n",
            "  Train Loss: 0.7257\n",
            "  Test Loss: 4.8307\n",
            "  Perplexity: 125.30\n",
            "Epoch 15/50:\n",
            "  Train Loss: 0.6652\n",
            "  Test Loss: 4.9251\n",
            "  Perplexity: 137.70\n",
            "Epoch 16/50:\n",
            "  Train Loss: 0.6372\n",
            "  Test Loss: 5.0576\n",
            "  Perplexity: 157.21\n",
            "Epoch 17/50:\n",
            "  Train Loss: 0.5732\n",
            "  Test Loss: 5.1320\n",
            "  Perplexity: 169.36\n",
            "Epoch 18/50:\n",
            "  Train Loss: 0.5391\n",
            "  Test Loss: 5.1826\n",
            "  Perplexity: 178.14\n",
            "Epoch 19/50:\n",
            "  Train Loss: 0.5269\n",
            "  Test Loss: 5.2936\n",
            "  Perplexity: 199.07\n",
            "Epoch 20/50:\n",
            "  Train Loss: 0.4933\n",
            "  Test Loss: 5.5434\n",
            "  Perplexity: 255.55\n",
            "\n",
            "Sample translations:\n",
            "EN: hello how are you -> AR: يمكنك شراؤه هنا\n",
            "EN: i am a student -> AR: كم الساعة\n",
            "EN: what is your name -> AR: كم الساعة\n",
            "\n",
            "Epoch 21/50:\n",
            "  Train Loss: 0.4222\n",
            "  Test Loss: 5.8597\n",
            "  Perplexity: 350.61\n",
            "Epoch 22/50:\n",
            "  Train Loss: 0.4584\n",
            "  Test Loss: 5.2118\n",
            "  Perplexity: 183.42\n",
            "Epoch 23/50:\n",
            "  Train Loss: 0.4044\n",
            "  Test Loss: 5.3123\n",
            "  Perplexity: 202.82\n",
            "Epoch 24/50:\n",
            "  Train Loss: 0.4548\n",
            "  Test Loss: 5.6378\n",
            "  Perplexity: 280.83\n",
            "Epoch 25/50:\n",
            "  Train Loss: 0.4141\n",
            "  Test Loss: 5.0880\n",
            "  Perplexity: 162.07\n",
            "Epoch 26/50:\n",
            "  Train Loss: 0.3763\n",
            "  Test Loss: 5.5857\n",
            "  Perplexity: 266.59\n",
            "Epoch 27/50:\n",
            "  Train Loss: 0.3585\n",
            "  Test Loss: 5.6223\n",
            "  Perplexity: 276.52\n",
            "Epoch 28/50:\n",
            "  Train Loss: 0.3259\n",
            "  Test Loss: 5.8285\n",
            "  Perplexity: 339.84\n",
            "Epoch 29/50:\n",
            "  Train Loss: 0.3553\n",
            "  Test Loss: 5.6419\n",
            "  Perplexity: 282.01\n",
            "Epoch 30/50:\n",
            "  Train Loss: 0.3048\n",
            "  Test Loss: 5.7338\n",
            "  Perplexity: 309.15\n",
            "\n",
            "Sample translations:\n",
            "EN: hello how are you -> AR: أحب أنت\n",
            "EN: i am a student -> AR: كم الساعة\n",
            "EN: what is your name -> AR: أحب السباحة\n",
            "\n",
            "Epoch 31/50:\n",
            "  Train Loss: 0.3623\n",
            "  Test Loss: 5.7917\n",
            "  Perplexity: 327.57\n",
            "Epoch 32/50:\n",
            "  Train Loss: 0.3502\n",
            "  Test Loss: 5.9511\n",
            "  Perplexity: 384.16\n",
            "Epoch 33/50:\n",
            "  Train Loss: 0.4176\n",
            "  Test Loss: 5.7364\n",
            "  Perplexity: 309.93\n",
            "Epoch 34/50:\n",
            "  Train Loss: 0.3213\n",
            "  Test Loss: 5.3163\n",
            "  Perplexity: 203.63\n",
            "Epoch 35/50:\n",
            "  Train Loss: 0.2765\n",
            "  Test Loss: 5.6835\n",
            "  Perplexity: 293.98\n",
            "Epoch 36/50:\n",
            "  Train Loss: 0.2629\n",
            "  Test Loss: 5.6989\n",
            "  Perplexity: 298.55\n",
            "Epoch 37/50:\n",
            "  Train Loss: 0.2402\n",
            "  Test Loss: 5.7382\n",
            "  Perplexity: 310.50\n",
            "Epoch 38/50:\n",
            "  Train Loss: 0.3238\n",
            "  Test Loss: 5.8270\n",
            "  Perplexity: 339.35\n",
            "Epoch 39/50:\n",
            "  Train Loss: 0.2696\n",
            "  Test Loss: 6.1872\n",
            "  Perplexity: 486.47\n",
            "Epoch 40/50:\n",
            "  Train Loss: 0.2071\n",
            "  Test Loss: 5.5860\n",
            "  Perplexity: 266.66\n",
            "\n",
            "Sample translations:\n",
            "EN: hello how are you -> AR: هل أنت جائع\n",
            "EN: i am a student -> AR: أتمنى أن تكون\n",
            "EN: what is your name -> AR: أتمنى أن تكون\n",
            "\n",
            "Epoch 41/50:\n",
            "  Train Loss: 0.2725\n",
            "  Test Loss: 6.0906\n",
            "  Perplexity: 441.68\n",
            "Epoch 42/50:\n",
            "  Train Loss: 0.2741\n",
            "  Test Loss: 6.0304\n",
            "  Perplexity: 415.88\n",
            "Epoch 43/50:\n",
            "  Train Loss: 0.2776\n",
            "  Test Loss: 5.4082\n",
            "  Perplexity: 223.22\n",
            "Epoch 44/50:\n",
            "  Train Loss: 0.2309\n",
            "  Test Loss: 5.7993\n",
            "  Perplexity: 330.06\n",
            "Epoch 45/50:\n",
            "  Train Loss: 0.2863\n",
            "  Test Loss: 5.9820\n",
            "  Perplexity: 396.23\n",
            "Epoch 46/50:\n",
            "  Train Loss: 0.2565\n",
            "  Test Loss: 6.2228\n",
            "  Perplexity: 504.11\n",
            "Epoch 47/50:\n",
            "  Train Loss: 0.2258\n",
            "  Test Loss: 6.2994\n",
            "  Perplexity: 544.24\n",
            "Epoch 48/50:\n",
            "  Train Loss: 0.2690\n",
            "  Test Loss: 6.1136\n",
            "  Perplexity: 451.98\n",
            "Epoch 49/50:\n",
            "  Train Loss: 0.2488\n",
            "  Test Loss: 6.2306\n",
            "  Perplexity: 508.05\n",
            "Epoch 50/50:\n",
            "  Train Loss: 0.2675\n",
            "  Test Loss: 5.8505\n",
            "  Perplexity: 347.41\n",
            "\n",
            "Sample translations:\n",
            "EN: hello how are you -> AR: يمكنك شراؤه هنا\n",
            "EN: i am a student -> AR: أتمنى\n",
            "EN: what is your name -> AR: كم الساعة\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
        "\n",
        "trg_pad_mask = (trg != 0).unsqueeze(1).unsqueeze(2)\n",
        "trg_pad_mask = trg_pad_mask.expand(2, 1, 8, 8)"
      ],
      "metadata": {
        "id": "7MT6__2orpMv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trg_pad_mask = (trg != 0).unsqueeze(1).unsqueeze(2)\n",
        "trg_pad_mask.shape"
      ],
      "metadata": {
        "id": "EF2p36elr2CP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb36716-c7c4-4a39-e61f-e54effa9de98"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1, 1, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTesting translations:\")\n",
        "test_sentences = [\n",
        "    \"انا طالب\",\n",
        "    \"what is your name\",\n",
        "    \"i am a student\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    translation = translate_sentence(model, sentence, src_vocab, tgt_vocab, device)\n",
        "    print(f\"EN: {sentence}\")\n",
        "    print(f\"FR: {translation}\")\n",
        "    print()\n",
        "\n"
      ],
      "metadata": {
        "id": "OWXkV3JJr6ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7997a59-de1b-4189-93dc-0cabf7ca843f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing translations:\n",
            "EN: انا طالب\n",
            "FR: أحب قراءة الكتب\n",
            "\n",
            "EN: what is your name\n",
            "FR: كم الساعة\n",
            "\n",
            "EN: i am a student\n",
            "FR: أتمنى\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ref:\n",
        "\n",
        "[Original paper](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "[Jay Alammar blog](https://jalammar.github.io/illustrated-transformer/)"
      ],
      "metadata": {
        "id": "9fQyu-SClkNY"
      }
    }
  ]
}
