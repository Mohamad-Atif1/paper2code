{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPMUdnkbym/kGtHNjvctktR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamad-Atif1/paper2code/blob/main/Transformers/ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This** notebook is made by **Eng. Mohammed Alshabrawi**"
      ],
      "metadata": {
        "id": "nzKOw__Y1X9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale**\n",
        "\n",
        "This is the second notebook for Transfromers. We will impelement vision transformer **(ViT)** from scratch!\n",
        "\n",
        "It is pretry simple if you understand the transformer architecture [Transformer from scratch](https://github.com/Mohamad-Atif1/paper2code/blob/main/Transformers/Transformer.ipynb)"
      ],
      "metadata": {
        "id": "JVKOFltP1imm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVLcgzpZsCcr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s7epBmJiayY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Images are 3D data (height, width, and color channels) while Transformers are designed for sequence 1D data (sequence of words or tokens). So, the first step is to change the number of image dimensions into a **sequence of 1D embeddings**.\n",
        "\n",
        "This achieved by:\n",
        "\n",
        "- Splitting the image into fixed-size, non-overlapping patches. For example, a 224x224 image might be divided into 196 patches, where each patch is 16x16 pixels.\n",
        "\n",
        "- Applying linear projection to each of these 2D patches. This learnable transformation maps the raw pixel values of each patch into a higher-dimensional 1D vector, known as a patch embedding.  This process is often  implemented using a Conv layer with a kernel size and stride equal to the patch size and the number of kernels is the embedding size\n",
        "\n",
        "- Finally, we will have sequence of 1D embeddings (196 patches)\n",
        "\n",
        "\n",
        "Also, we will add Positional Embedding because Transformers look at all parts of a sequence at once, without knowing the position of the patches. In the original paper, Positional Embedding vectors are learnable vectors\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*tjEPjCT4Os-mRI3cTn0jVg.png\" height=400>"
      ],
      "metadata": {
        "id": "vE6BsQGI_bIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self,img_size,patch_size,embed_size,in_channels):\n",
        "        super(PatchEmbedding,self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        # number of patches = (Height * Width) / (patch_height * patch_width)\n",
        "        # in the orginal paper, images and patches are square (Height = Width), so\n",
        "        # number of patches = (Highet^2) / (patch_height^2) = (Highet / patch_height) ^2\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "        assert img_size % patch_size == 0, f\"Image size {img_size} must be divisible by patch size {patch_size}\"\n",
        "\n",
        "        # Extracting \"embed_size\" features of each patch seperatelly using Conv2d\n",
        "        # In the paper it is named \" Linear Projection of Flattend Patches\"\n",
        "        self.projection = nn.Conv2d(in_channels,embed_size,patch_size,patch_size)\n",
        "\n",
        "    def forward(self,img):\n",
        "        img = self.projection(img) # Now we have n features for each patch, n = embed_size\n",
        "        img = img.reshape(-1,self.n_patches,self.embed_size) # bs, seq_len, embed_size\n",
        "        return img"
      ],
      "metadata": {
        "id": "Inf7asE5y8YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will pass these sequence of embedding into the encoder. But before doing that, we will insert an **extra learnable embedding** beside patches embedding known as class token \\<CLS\\>. Its purpose is to gather global information from the entire image as it passes through the Transformer Encoder layers.\n",
        "\n",
        "We will classifer the images by passing the \\<CLS\\> token to a classifer (MLP head)\n",
        "\n",
        "For Transformer Encoder layer, we will use nn.TransformerEncoderLayer"
      ],
      "metadata": {
        "id": "NwOv1DNRTzze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2022/02/vit_1.png\" height=500 width=800 >"
      ],
      "metadata": {
        "id": "bEffz-j8Ky69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            img_size,\n",
        "            patch_size,\n",
        "            embed_size,\n",
        "            in_channels,\n",
        "            num_heads,\n",
        "            num_layers,\n",
        "            ff_expansion,\n",
        "            dout,\n",
        "            classes,\n",
        "            ):\n",
        "        super(ViT,self).__init__()\n",
        "\n",
        "        self.n_patches = (img_size // patch_size) ** 2 # HW/P^2\n",
        "        assert img_size % patch_size == 0, f\"Image size {img_size} must be divisible by patch size {patch_size}\"\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches + 1 , embed_size)) # + 1 for CLS token\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,embed_size))\n",
        "        self.patch_embed = PatchEmbedding(img_size,patch_size,embed_size,in_channels)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "                    d_model=embed_size,\n",
        "                    nhead=num_heads,\n",
        "                    dim_feedforward=int(embed_size * ff_expansion),\n",
        "                    dropout=dout,\n",
        "                    activation='gelu', # They used gelu in ViT paper\n",
        "                    batch_first=True,\n",
        "                    norm_first=True  # pre-norm architecture like ViT paper\n",
        "                )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer,num_layers)\n",
        "        self.mlp_head = nn.Linear(embed_size,classes)\n",
        "\n",
        "\n",
        "    def forward(self,img):\n",
        "        img = self.patch_embed(img)\n",
        "        cls = self.cls.expand(img.size(0),-1,-1) # bs,1,embed_size\n",
        "        img = torch.cat([cls, img], dim=1) # add one more patch to the image (cls patch)\n",
        "\n",
        "        # You can expand the pos_embed to match batch size, but\n",
        "        # pytorch do it automatically when only one dimension is different\n",
        "        img = img + self.pos_embed # this broadcasting automatically\n",
        "\n",
        "        # Now we are ready to pass these patches to the Encoder\n",
        "        img = self.encoder(img)\n",
        "\n",
        "        # Only pass the Cls token to the output layer\n",
        "        out = self.mlp_head(img[:,0,:])\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "PtF8eTV2zGWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let us test it on MNIST"
      ],
      "metadata": {
        "id": "vxcm-kUS-lij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # MNIST mean and std\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "G4D7xwXpkBcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ViT(\n",
        "            img_size=28,\n",
        "            patch_size=7,\n",
        "            embed_size=510,\n",
        "            in_channels=1,\n",
        "            num_heads=6,\n",
        "            num_layers=3,\n",
        "            ff_expansion=4,\n",
        "            dout=0.2,\n",
        "            classes=10,\n",
        "\n",
        ").to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "print(f\"Device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OZDr8GolAFC",
        "outputId": "b74e6757-a269-4fb0-ecf2-439e19d72b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has 9423280 parameters\n",
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def train(model,criterion,train_loader,test_loader,num_epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for data, target in tqdm(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "        accuracy = 100. * correct / total\n",
        "        print(f'Epoch {epoch+1}: Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in tqdm(test_loader):\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                test_loss += criterion(output, target).item()\n",
        "                pred = output.argmax(dim=1)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "                total += target.size(0)\n",
        "\n",
        "        test_accuracy = 100. * correct / total\n",
        "        print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "KnjdSXBE-ir_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model,criterion,train_loader,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsiURTb-nKyq",
        "outputId": "7e2c4ca1-c3e3-4527-909c-3ad68c1abb9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:20<00:00, 22.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss: 0.4906, Accuracy: 86.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:02<00:00, 32.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 93.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:20<00:00, 22.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss: 0.1618, Accuracy: 95.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:02<00:00, 32.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 95.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:20<00:00, 22.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss: 0.1370, Accuracy: 95.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:02<00:00, 32.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 95.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:20<00:00, 22.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss: 0.1378, Accuracy: 95.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:02<00:00, 33.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 96.51%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:20<00:00, 22.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Loss: 0.1205, Accuracy: 96.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:02<00:00, 33.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 95.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK now everything works fine. Let us use our own [Encoder](https://github.com/Mohamad-Atif1/paper2code/blob/main/Transformers/Transformer.ipynb) instead of using **nn.TransformerEncoderLayer**"
      ],
      "metadata": {
        "id": "FnGaEnd_JaR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VIT with custom Encoder"
      ],
      "metadata": {
        "id": "ChR_0Ww8qVSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,embed_size,heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "\n",
        "        assert (self.head_dim * heads == embed_size), \"embed_size // heads is not divisible\"\n",
        "\n",
        "        self.queries = nn.Linear(self.embed_size,self.embed_size)\n",
        "        self.keys = nn.Linear(self.embed_size,self.embed_size)\n",
        "        self.values = nn.Linear(self.embed_size,self.embed_size)\n",
        "\n",
        "        self.fc = nn.Linear(self.embed_size, self.embed_size)\n",
        "\n",
        "    def forward(self,q,k,v,mask):\n",
        "        q = self.queries(q)\n",
        "        k = self.keys(k)\n",
        "        v = self.values(v)\n",
        "        # split the embedding into k heads\n",
        "        # q,v,k shapes: [bs, num_of_tokens (N) , heads * head_dim ]\n",
        "        # Now techincally, we will work on each head independently, as if there were batches of heads!\n",
        "        # (batch_size * heads, num_tokens, head_dim)\n",
        "        q = q.reshape(q.shape[0]*self.heads, q.shape[1], self.head_dim)\n",
        "        k = k.reshape(k.shape[0]*self.heads, k.shape[1], self.head_dim)\n",
        "        v = v.reshape(v.shape[0]*self.heads, v.shape[1], self.head_dim)\n",
        "\n",
        "        energy = torch.bmm(q,k.permute(0,2,1)) # Q*K^T -> bs, q_N, k_N\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask==0,float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy/(self.head_dim ** (0.5)), dim=2)\n",
        "        #\n",
        "\n",
        "        out = torch.bmm(attention,v)\n",
        "        out = out.reshape(q.shape[0]//self.heads, q.shape[1], self.embed_size) # bs,N,h*heads\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pYMkp8pBqgXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,embed_size,heads, ff_expantion,dout=0.1):\n",
        "        super(EncoderBlock,self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_size,heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size,embed_size*ff_expantion),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_size*ff_expantion,embed_size))\n",
        "        self.dout = nn.Dropout(dout)\n",
        "\n",
        "\n",
        "    def forward(self,q,k,v,mask):\n",
        "        sub_layer_one = self.attention(q,k,v,mask)\n",
        "        sub_layer_one = self.dout(sub_layer_one)\n",
        "        sub_layer_one += q # skip connection\n",
        "        sub_layer_one = self.norm1(sub_layer_one)\n",
        "\n",
        "        sub_layer_two = self.feed_forward(sub_layer_one)\n",
        "        sub_layer_two = self.dout(sub_layer_two)\n",
        "        sub_layer_two += sub_layer_one\n",
        "        sub_layer_two = self.norm2(sub_layer_two)\n",
        "\n",
        "\n",
        "\n",
        "        return sub_layer_two\n"
      ],
      "metadata": {
        "id": "_CYYZA0yqjCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            img_size,\n",
        "            patch_size,\n",
        "            embed_size,\n",
        "            in_channels,\n",
        "            num_heads,\n",
        "            num_layers,\n",
        "            ff_expansion,\n",
        "            dout,\n",
        "            classes,\n",
        "            ):\n",
        "        super(ViT,self).__init__()\n",
        "\n",
        "        self.n_patches = (img_size // patch_size) ** 2 # HW/P^2\n",
        "        assert img_size % patch_size == 0, f\"Image size {img_size} must be divisible by patch size {patch_size}\"\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches + 1 , embed_size)) # + 1 for CLS token\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,embed_size))\n",
        "        self.patch_embed = PatchEmbedding(img_size,patch_size,embed_size,in_channels)\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            self.layers.append(EncoderBlock(embed_size,num_heads, ff_expansion,dout=0.1))\n",
        "        self.mlp_head = nn.Linear(embed_size,classes)\n",
        "\n",
        "\n",
        "    def forward(self,img):\n",
        "        img = self.patch_embed(img)\n",
        "        cls = self.cls.expand(img.size(0),-1,-1) # bs,1,embed_size\n",
        "        img = torch.cat([cls, img], dim=1) # add one more patch to the image (cls patch)\n",
        "\n",
        "        # You can expand the pos_embed to match batch size, but\n",
        "        # pytorch do it automatically when only one dimension is different\n",
        "        img = img + self.pos_embed # this broadcasting automatically\n",
        "\n",
        "        # Now we are ready to pass these patches to the Encoder\n",
        "        for layer in self.layers:\n",
        "            img = layer(img,img,img,None)\n",
        "\n",
        "        # Only pass the Cls token to the output layer\n",
        "        out = self.mlp_head(img[:,0,:])\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "U6E7Nd8WqrVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ViT(\n",
        "            img_size=28,\n",
        "            patch_size=7,\n",
        "            embed_size=510,\n",
        "            in_channels=1,\n",
        "            num_heads=6,\n",
        "            num_layers=3,\n",
        "            ff_expansion=4,\n",
        "            dout=0.2,\n",
        "            classes=10,\n",
        "\n",
        ").to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAGXGNpZrVCu",
        "outputId": "c894f19b-64f1-47b7-ab2d-6bd73a402a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has 9423280 parameters\n",
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def train(model,criterion,train_loader,test_loader,num_epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for data, target in tqdm(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "        accuracy = 100. * correct / total\n",
        "        print(f'Epoch {epoch+1}: Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in tqdm(test_loader):\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                test_loss += criterion(output, target).item()\n",
        "                pred = output.argmax(dim=1)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "                total += target.size(0)\n",
        "\n",
        "        test_accuracy = 100. * correct / total\n",
        "        print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "fACg46Rsrclb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model,criterion,train_loader,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnQ5UiFNrgn4",
        "outputId": "8079357b-9c7f-448a-f795-192e220b4a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:19<00:00, 23.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss: 0.5213, Accuracy: 84.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:02<00:00, 33.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 92.42%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:19<00:00, 23.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss: 0.5855, Accuracy: 82.64%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:02<00:00, 34.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 92.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:19<00:00, 23.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss: 0.2794, Accuracy: 91.96%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:02<00:00, 34.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 91.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:19<00:00, 23.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss: 0.2376, Accuracy: 93.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:02<00:00, 34.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 94.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:19<00:00, 23.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Loss: 0.2097, Accuracy: 94.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:02<00:00, 34.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 95.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that our custom encoder works perfectly! ✅\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "7gWmMvt0KG52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REF**\n",
        "\n",
        "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)"
      ],
      "metadata": {
        "id": "DqNR4NW4Khgo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rRlxl8Mzr78t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}