{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-W5hA9R0YuNV",
        "3TTIG3BWY0NW",
        "gOn9a1sLZGXx",
        "Hay5tQfPZH_L"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Vector-Quantized Variational Autoencoder (VQ-VAE)**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1yU3x4R2kBQCxUmUy2NzYjhQhRBi2ljpb\" width=\"1000\"/>\n",
        "\n",
        "original paper: [Neural Discrete Representation Learning (2017)](https://arxiv.org/abs/1711.00937)\n",
        "\n",
        "notebook explained & made by: [**Anas Aldadi**](https://github.com/AnasKAN)\n",
        "\n",
        "[open on colab](https://colab.research.google.com/drive/1TzwFDzmmQD9uTG1aYrB09yTcY3_9SDaQ?usp=sharing)"
      ],
      "metadata": {
        "id": "lmqHxNH4VLWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "VQ-VAEs simply put have a discrete latent space in other words it means the encoder output discrete codes (removing the property of having infinite possible latent vectors) unlike VAEs which have continuous latent space, and the prior is learnt rather than being static.\n",
        "\n",
        "But Why? and how would we achieve that?\n",
        "\n",
        "For the why, VQ-VAE came to solve the issue of \"posterior collapse\" which VAEs with powerful decoders suffer from, the decoder simply ignore the latents (the changes in latent space doesn't influence the generation).\n",
        "\n",
        "Not only that, it is the first archeticture that offers discrete latent space, and utilize the latent space better than previous discretizing attempts, because it provides similar results to the continuous counterpart.\n",
        "\n",
        "a direct qoutation from the paper on the contributions of this archeticture:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Qg7gZtXihRLSEJ5DB0U46OD4McYgKNlQ\" width=\"1000\"/>"
      ],
      "metadata": {
        "id": "MP_0q84mVeqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Approach**\n",
        "\n",
        "I'll explain each component in details in this section:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1gwzB-59J5YG5Bsr0Ktae0xHqOuCMuJMt\" width=\"1000\"/>\n",
        "\n",
        "Figure 3: The whole archeticture.\n",
        "\n",
        "As the figure illustrates the archeticture consists of 3 trainable components namely the encoder, the decoder and the codebook.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1xP1HJBx4h71Il4xzrDXUS0-rorN_G2Hb\" width=\"700\"/>\n",
        "\n",
        "Figure 4: The encoder\n",
        "\n",
        "The encoder takes the image let's say 64,64,3 and compress it into a latent continuous space let's say 4,4,64.\n",
        "\n",
        "The encoder can be built using Conv layers, MLP layers, RNN layers, or Transformer layers it really up to you.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=18-WPL_rVPPsu8md61OeCYGLJb0blSTtq\" width=\"700\"/>\n",
        "\n",
        "Figure 5: The Vector Quantizer\n",
        "\n",
        "The codebook is defined with two parameters the dimension of the latent space $D$ (which is exactly the number of kernels/filters in the last encoder layer), and the number of latent vectors/codewords $K$, each codebook in literature is written with notation $e_K$.\n",
        "\n",
        "Codebooks are essentially set of multiple learnable latent vectors, this way we discretize the latent space instead of having continuous latent vectors that can have any value, now we force the model to choose only one set that is the closest to $Z_e(x)$ using any distance measuring equation.\n",
        "\n",
        "This chosen codeword (might be $e_1$, $e_2$, ..., $e_K$) becomes the quantized latent $Z_q(x)$.\n",
        "\n",
        "So, $Z_q(x)$ $=$ $e_K$ where $K$ $=$ $argmin_j{\\|Z_e-e_j\\|_2}$. we literally just copy the chosen codeword and give it to the decoder.\n",
        "\n",
        "As you might have noticed! there is no random sampling!! that's one of the key differences between VAEs and VQVAEs.\n",
        "\n",
        "while VAEs have gaussian $q_\\phi(Z|x)$, VQVAEs have deterministic $q_\\phi(Z|x)$.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1R2R4FvYdYVdSdQO8xl0BXnL4c-bZVsjv\" width=\"700\"/>\n",
        "\n",
        "Figure 6: The Vector Quantizer\n",
        "\n",
        "This begs the question how will we deal with this deterministic sampling when VAEs are probabilistic archeticture natively?\n",
        "\n",
        "In VAEs since the prior is gaussain it biases the latent posterior so it will also be gaussian due to the KL divergence term encourage it to be so.\n",
        "\n",
        "VQVAEs instead uses categorical distribution $q(e_K|X)$, and since the procedure of choosing a codeword index $K$ is deterministic (neighrest neighbor) so that means the posterior will have a probability of 1 for the category that is chosen:\n",
        "\n",
        "$q(Z_q = e_K|X)$ $=$ $\\begin{cases} 1 & \\text{for } K = \\underset{j}{\\operatorname{argmin}} \\|Z_e - e_j\\|_2 \\\\ 0 & \\text{otherwise} \\end{cases}$\n",
        "\n",
        "What about the prior? during training we don't want our model to bias the model into the indices it chooses. So the prior is just a uniform categorical distribution, with each indices having $p(Z_q=e_K) = 1/K$ where $K$ is the codebook size.\n",
        "\n",
        "The KL Divergence loss will look from:\n",
        "\n",
        "$D_KL(P(X)\\|Q(X)) = \\Sigma_x P(X) log(\\frac{P(X)}{Q(X)})$\n",
        "\n",
        "simplified to:\n",
        "\n",
        "$D_KL(q(Z_q|X)\\|p(Z_q)) = \\Sigma_x q(Z_q|X) log(\\frac{q(Z_q|X)}{p(Z_q)})$ $=$ $log(\\frac{1}{\\frac{1}{K}})$ $=$ $log(K)$\n",
        "\n",
        "because remember posterior is one at chosen $K$ and prior is $\\frac{1}{K}$ at $K$\n",
        "\n",
        "Although in theory we assume fixed prior but after training we might find the joint distribution of codewords across our dataset is not a uniform distribution, why is that? There is a true prior that is learnt during training. For example we might find that for a certain data (let's say a pic of bird) when sampling (neighrest neighbor) codebooks to build the quantized vectors for location 4 let's say $Z_q4$ condition the distribution of $Z_q2$ so $p(Z_q4|Z_q2) \\not = p(Z_q4)$ and we stray from this distribution we will generate something other than a bird.\n",
        "\n",
        "This is like in language modelling how some words depend on previous words, here some codewords depend on previous codewords.\n",
        "\n",
        "Although the model learnt some true prior, the problem is we don't know what it is.\n",
        "\n",
        "How to mitigate this issue? use secondary autoregressive model to learn the appropriate probability distribution over codewords.\n",
        "\n",
        "In a nutshell, the prior distribution in VAEs $p_\\theta(z)$ is predefined, in VQVAEs we have a true prior distribution that is learnt implicitly during training!\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1N7W3KBoyrExJsplsw_uQ_wVVYPNsHWKi\" width=\"700\"/>\n",
        "\n",
        "Figure 7: The Decoder\n",
        "\n",
        "It takes the discrete latent and generate $y$. (idk its as simple as this).\n",
        "\n",
        "Now after decoding we goes to compute the loss function, and as we saw before the loss function doesn't have KL divergence.\n",
        "but we do have recon term in form of negative log likelihood, and the full form of the loss is dependent on all 3 components of the archeticture (to ensure every component learn correctly justly and its just a beautiful sensible loss).\n",
        "\n",
        "Notice there is no expectation over the posterior bec its deterministic. \\\\\n",
        "$L(\\theta,\\phi, C; x, Z_q) = - log(p_\\phi(x|Z_q)) + ...$ \\\\\n",
        "`________________________^ reconstruction term` it optimizes $\\theta, \\phi$ \\\\\n",
        "The encoder parameterized by $\\phi$ \\\\\n",
        "The decoder parameterized by $\\theta$ \\\\\n",
        "The codebook parameterized by $C$ \\\\\n",
        "\n",
        "If you might have guessed already, the quantization step is not differentiable. To solve this dilemma we gonna directly copy the gradients $\\nabla L$ from $Z_q$ to $Z_e$. We simply passed the gradients *Straight-through* the quantization layer (and this is why this trick is called Straight-through estimation).\n",
        "\n",
        "Straight-through estimation:\n",
        "\n",
        "$\\frac{\\partial \\nabla L}{\\partial Z_e}$ $=$ $\\frac{\\partial \\nabla L}{\\partial Z_q}$\n",
        "\n",
        "This would only make since if $Z_e \\approx Z_q$ and that's why neighrest neighbor make since.\n",
        "\n",
        "Althought this solves the issue of passing the gradients to the encoder and now it can learn, but the codebooks (which are randomly initialized) are not getting any gradients and this will limit the ability to control the generation if codebooks remain static.\n",
        "\n",
        "So in order to optimize codebooks we have to introduce the second loss term:\n",
        "\n",
        "$L(\\theta,\\phi, C; x, Z_q) = ... + \\|sg[Z_e(x)]-e\\|^2_2 + ...$ \\\\\n",
        "`________________________^ codebook loss` it optimizes $C$  \\\\\n",
        "\n",
        "Simply penalizes each codeword vector $e$ based on the squared distance from its associated encoding vector $Z_e(x)$\n",
        "\n",
        "Notice $sg$ (stop gradient) operator, it just set the gradient of anything inside it to zero. In our case its the gradients of encoding vectors, so it doesn't provide any gradient signal to the encoder (the encoder doesn't benfit from this term).\n",
        "\n",
        "Stopping here will lead to some problems:\n",
        "\n",
        "1. Fluctuations between code vectors:\n",
        "    This happens when the encoder trains more quickly than the codebook, they lead to instability and inefficient codebook utilization (redundent codewords)\n",
        "\n",
        "2. Arbitrary growth of encoding $Z_e$\n",
        "    Reconstruction loss might encorage higher values of $Z_e$ leading to also growth of some codeword $e$ which leads to instability and since the codebook loss doesn't affect the encoder, it doesn't encourage the $Z_e$ to have values closer to codewords.\n",
        "\n",
        "Third term came to rescue. Commitment loss \\\\\n",
        "$L(\\theta,\\phi, C; x, Z_q) = ... + ... + \\|Z_e(x)-sg[e]\\|^2_2$ \\\\\n",
        "`____________________________^ commitment loss` it optimizes $\\phi$ \\\\\n",
        "\n",
        "it looks like the codebook loss $\\|sg[Z_e(x)]-e\\|^2_2$ but the stop gradient operator on a different location.\n",
        "\n",
        "Great [source](https://youtu.be/yQvELPjmyn0?si=3RupDuLG_yB67fGa) to understand the math.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1RjJcnIUxzIWw0HMpqmmWXdJzEWSy3xi8\" width=\"1000\"/>\n",
        "\n",
        "Figure 8 [source](https://paperswithcode.com/method/vq-vae)\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "As shown in the figure above the archeticture has 3 main components, which are:\n",
        "\n",
        "-   ```\n",
        "    Encoder: the same encoder we all know and love, I'll add residuals to match\n",
        "    the paper description. (this will output the same continuous latent space)\n",
        "    ```\n",
        "Note: This continuous latent is denoted as $z_e$\n",
        "\n",
        "-   ```\n",
        "    The middle part is \"Vector Quantizer Layer\" (Codebook space): simply it is\n",
        "    a set of randomly initialized vectors that we will train and hope each one\n",
        "    resemble a set of group in the data. And we will measure the L2 euclidean\n",
        "    distance (or any method to measure distance) between codebooks and the\n",
        "    continuous latent outputted from the encoder.\n",
        "    ```\n",
        "    -   Codebook: The VQ layer maintains a learnable \"codebook\" or \"embedding space\" which is a matrix of K discrete embedding vectors, each of dimension D (where D is num_filters from the encoder output). Each $e_i$ in the codebook is a vector in $R^D$.\n",
        "\n",
        "    -   Quantization Process: For each vector in the encoder's output ($z_e$), the VQ layer finds the closest embedding vector $e_j$ in the codebook using a distance metric, typically Euclidean distance (L2-normalized distance). The encoder's output is then replaced by this closest codebook vector. This effectively \"quantizes\" the continuous representation into a discrete index.\n",
        "\n",
        "    Note: This discrete latent is denoted as $z_q$ in the figure\n",
        "\n",
        "    -   Straight-Through Estimator: A crucial aspect of VQ-VAE is handling the non-differentiability of the argmin operation (finding the closest vector). To allow gradients to flow back to the encoder, a \"straight-through estimator\" is used. This means that during the backward pass, *the gradients from the decoder are directly copied to the encoder's output*, bypassing the quantization step. The intuition is that since the encoder's output and the decoder's input share the same dimensional space, the gradients still provide useful information for the encoder to adjust its output.\n",
        "\n",
        "\n",
        "-   ```\n",
        "    Decoder: will take the codebook (or quantized discrete latent\n",
        "    representation) that has been chosen by the L2 euclidean distance\n",
        "    and generate that.\n",
        "    ```"
      ],
      "metadata": {
        "id": "_n2fcPT7qFQa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2A_dGDJkaN1O",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Reqs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from IPython.display import Image as IPImage, display\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helpers"
      ],
      "metadata": {
        "id": "-W5hA9R0YuNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_conv2d_output_dim(input_size, kernel_size, stride, padding, dilation=1):\n",
        "    \"\"\"\n",
        "    Calculate the output height or width of a 2D convolution layer.\n",
        "\n",
        "    Parameters:\n",
        "    - input_size (int): Size of input height or width\n",
        "    - kernel_size (int): Size of the convolution kernel\n",
        "    - stride (int): Stride of the convolution\n",
        "    - padding (int): Padding added to the input\n",
        "    - dilation (int): Dilation rate (default is 1)\n",
        "\n",
        "    Returns:\n",
        "    - int: Output dimension (height or width)\n",
        "    \"\"\"\n",
        "    return ((input_size + 2 * padding - dilation * (kernel_size - 1) - 1) // stride) + 1\n",
        "\n",
        "def calc_transpose_conv2d_output_dim(input_size, kernel_size, stride, padding, output_padding=0):\n",
        "    \"\"\"\n",
        "    Calculate the output height or width of a 2D transposed convolution layer.\n",
        "\n",
        "    Parameters:\n",
        "    - input_size (int): Size of input height or width\n",
        "    - kernel_size (int): Size of the convolution kernel\n",
        "    - stride (int): Stride of the convolution\n",
        "    - padding (int): Padding added to the input\n",
        "    - output_padding (int): Additional size added to one side of the output (default 0)\n",
        "\n",
        "    Returns:\n",
        "    - int: Output dimension (height or width)\n",
        "    \"\"\"\n",
        "    return (input_size - 1) * stride - 2 * padding + kernel_size + output_padding"
      ],
      "metadata": {
        "id": "ve3qm2hOhR-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_img_grid(img_tensors, nrows, filename):\n",
        "    \"\"\"\n",
        "    Save a grid of image tensors in [0, 1] range as <filename>.jpg\n",
        "    \"\"\"\n",
        "    img_tensors = img_tensors.clamp(0, 1).permute(0, 2, 3, 1).cpu().numpy()\n",
        "    img_tensors = (img_tensors * 255).astype(np.uint8)\n",
        "\n",
        "    b, h, w = img_tensors.shape[:3]\n",
        "    ncols   = b // nrows\n",
        "    grid    = np.zeros((nrows*h, ncols*w, 3), dtype=np.uint8)\n",
        "\n",
        "    for idx, img in enumerate(img_tensors):\n",
        "        r, c = divmod(idx, ncols)\n",
        "        grid[r*h:(r+1)*h, c*w:(c+1)*w] = img\n",
        "\n",
        "    Image.fromarray(grid, mode=\"RGB\").save(f\"{filename}.jpg\")"
      ],
      "metadata": {
        "id": "ClxeiYF0YvXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Data"
      ],
      "metadata": {
        "id": "3TTIG3BWY0NW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "print(f'num of workers {NUM_WORKERS}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKihAIYTY1ed",
        "outputId": "abfb651c-47dd-437c-a28d-0b5f270936bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num of workers 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ROOT = \"./data\"\n",
        "\n",
        "transform  = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_ds = CIFAR10(DATA_ROOT, train=True,  transform=transform, download=True)\n",
        "test_ds  = CIFAR10(DATA_ROOT, train=False, transform=transform, download=True)\n",
        "\n",
        "train_var = np.var(train_ds.data / 255.0)   # used to scale the loss\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
        "                          shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE,\n",
        "                          shuffle=False, num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "pyajpGGEEu96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. VQ-VAE Archeticture"
      ],
      "metadata": {
        "id": "h7StL2odYwQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 Encoder"
      ],
      "metadata": {
        "id": "gOn9a1sLZGXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU()\n",
        "    )"
      ],
      "metadata": {
        "id": "KgcMwv3EgfpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, num_channels, num_filters):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encode = nn.Sequential(\n",
        "            conv_block(num_channels, num_filters*2),            # 32x32\n",
        "            conv_block(num_filters*2, num_filters*4, stride=2), # 16x16\n",
        "            conv_block(num_filters*4, num_filters*8, stride=2), # 8x8\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.encode(input)"
      ],
      "metadata": {
        "id": "1W8iQPepZHvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Decoder"
      ],
      "metadata": {
        "id": "Hay5tQfPZH_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deconv_block(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU()\n",
        "    )"
      ],
      "metadata": {
        "id": "4y_nt1X7go8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, num_filters, num_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decode = nn.Sequential(\n",
        "            nn.Conv2d(embedding_dim, num_filters * 8, kernel_size=1),  # 8x8\n",
        "            deconv_block(num_filters*8, num_filters*4),                # 16x16\n",
        "            conv_block(num_filters*4, num_filters*4),\n",
        "\n",
        "            deconv_block(num_filters*4, num_filters*2),                # 32x32\n",
        "            conv_block(num_filters*2, num_filters*2),\n",
        "\n",
        "            nn.Conv2d(num_filters*2, num_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, codebook):\n",
        "        return self.decode(codebook)"
      ],
      "metadata": {
        "id": "O1Vo_AV-ZJez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 Vector Quantizer"
      ],
      "metadata": {
        "id": "CaUVEy16pnYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i took the implementation of the VectorQuantizer layer and VQVAE Wrapper class from https://github.com/airalcorn2/vqvae-pytorch/blob/master/vqvae.py"
      ],
      "metadata": {
        "id": "GyWoua9oDAuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SonnetExponentialMovingAverage(nn.Module):\n",
        "    # See: https://github.com/deepmind/sonnet/blob/5cbfdc356962d9b6198d5b63f0826a80acfdf35b/sonnet/src/moving_averages.py#L25.\n",
        "    # They do *not* use the exponential moving average updates described in Appendix A.1\n",
        "    # of \"Neural Discrete Representation Learning\".\n",
        "    def __init__(self, decay, shape):\n",
        "        super().__init__()\n",
        "        self.decay = decay\n",
        "        self.counter = 0\n",
        "        self.register_buffer(\"hidden\", torch.zeros(*shape))\n",
        "        self.register_buffer(\"average\", torch.zeros(*shape))\n",
        "\n",
        "    def update(self, value):\n",
        "        self.counter += 1\n",
        "        with torch.no_grad():\n",
        "            self.hidden -= (self.hidden - value) * (1 - self.decay)\n",
        "            self.average = self.hidden / (1 - self.decay ** self.counter)\n",
        "\n",
        "    def __call__(self, value):\n",
        "        self.update(value)\n",
        "        return self.average"
      ],
      "metadata": {
        "id": "OmK095ilu2zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, use_ema, decay, epsilon):\n",
        "        super().__init__()\n",
        "        # See Section 3 of \"Neural Discrete Representation Learning\" and:\n",
        "        # https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L142.\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.use_ema = use_ema\n",
        "        # Weight for the exponential moving average.\n",
        "        self.decay = decay\n",
        "        # Small constant to avoid numerical instability in embedding updates.\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Dictionary embeddings.\n",
        "        limit = 3 ** 0.5\n",
        "        e_i_ts = torch.FloatTensor(embedding_dim, num_embeddings).uniform_(\n",
        "            -limit, limit\n",
        "        )\n",
        "        if use_ema:\n",
        "            self.register_buffer(\"e_i_ts\", e_i_ts)\n",
        "        else:\n",
        "            self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
        "\n",
        "        # Exponential moving average of the cluster counts.\n",
        "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
        "        # Exponential moving average of the embeddings.\n",
        "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        flat_x = x.permute(0, 2, 3, 1).contiguous().reshape(-1, self.embedding_dim)\n",
        "        \"\"\"\n",
        "        Ok here you might wonder how exactly it is better to return the C to\n",
        "        its original place instead of the pytorch default help calculations?\n",
        "        the answer is for every latent we get from the encoder we have (H*W) for every C (or kernel).\n",
        "        and performing mult and sum over the first 2 dims after the batch is better than jumping\n",
        "        right off C to get to the dim space of H and W.\n",
        "        as for .contiguous() is important after permute() because permute only changes the view of\n",
        "        the tensor, not its underlying memory layout. contiguous() ensures that the tensor is stored\n",
        "        contiguously in memory in the new order\n",
        "        \"\"\"\n",
        "        distances = (\n",
        "            (flat_x ** 2).sum(1, keepdim=True)\n",
        "            - 2 * flat_x @ self.e_i_ts\n",
        "            + (self.e_i_ts ** 2).sum(0, keepdim=True)\n",
        "        )\n",
        "        encoding_indices = distances.argmin(1)\n",
        "        quantized_x = F.embedding(\n",
        "            encoding_indices.view(x.shape[0], *x.shape[2:]), self.e_i_ts.transpose(0, 1)\n",
        "        ).permute(0, 3, 1, 2)\n",
        "\n",
        "        # See second term of Equation (3).\n",
        "        if not self.use_ema:\n",
        "            dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
        "        else:\n",
        "            dictionary_loss = None\n",
        "\n",
        "        # See third term of Equation (3).\n",
        "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
        "        # Straight-through gradient. See Section 3.2.\n",
        "        quantized_x = x + (quantized_x - x).detach()\n",
        "\n",
        "        if self.use_ema and self.training:\n",
        "            with torch.no_grad():\n",
        "                # See Appendix A.1 of \"Neural Discrete Representation Learning\".\n",
        "\n",
        "                # Cluster counts.\n",
        "                encoding_one_hots = F.one_hot(\n",
        "                    encoding_indices, self.num_embeddings\n",
        "                ).type(flat_x.dtype)\n",
        "                n_i_ts = encoding_one_hots.sum(0)\n",
        "                # Updated exponential moving average of the cluster counts.\n",
        "                # See Equation (6).\n",
        "                self.N_i_ts(n_i_ts)\n",
        "\n",
        "                # Exponential moving average of the embeddings. See Equation (7).\n",
        "                embed_sums = flat_x.transpose(0, 1) @ encoding_one_hots\n",
        "                self.m_i_ts(embed_sums)\n",
        "\n",
        "                # This is kind of weird.\n",
        "                # Compare: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L270\n",
        "                # and Equation (8).\n",
        "                N_i_ts_sum = self.N_i_ts.average.sum()\n",
        "                N_i_ts_stable = (\n",
        "                    (self.N_i_ts.average + self.epsilon)\n",
        "                    / (N_i_ts_sum + self.num_embeddings * self.epsilon)\n",
        "                    * N_i_ts_sum\n",
        "                )\n",
        "                self.e_i_ts = self.m_i_ts.average / N_i_ts_stable.unsqueeze(0)\n",
        "\n",
        "        return (\n",
        "            quantized_x,\n",
        "            dictionary_loss,\n",
        "            commitment_loss,\n",
        "            encoding_indices.view(x.shape[0], -1),\n",
        "        )"
      ],
      "metadata": {
        "id": "aA72WUELpqfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 VQVAE"
      ],
      "metadata": {
        "id": "e4Sjp7ldZJ8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title I simplified the implementation of the original code\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_channels,\n",
        "        num_filters,\n",
        "        embedding_dim,\n",
        "        num_embeddings,\n",
        "        use_ema,\n",
        "        decay,\n",
        "        epsilon,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            num_channels,\n",
        "            num_filters,\n",
        "        )\n",
        "        self.pre_vq_conv = nn.Conv2d(\n",
        "            in_channels=num_filters*8 ,out_channels=embedding_dim, kernel_size=1\n",
        "        )\n",
        "        self.vq = VectorQuantizer(\n",
        "            embedding_dim, num_embeddings, use_ema, decay, epsilon\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            embedding_dim,\n",
        "            num_filters,\n",
        "            num_channels,\n",
        "        )\n",
        "\n",
        "    def quantize(self, x):\n",
        "        z = self.pre_vq_conv(self.encoder(x))\n",
        "        (z_quantized, dictionary_loss, commitment_loss, encoding_indices) = self.vq(z)\n",
        "        return (z_quantized, dictionary_loss, commitment_loss, encoding_indices)\n",
        "\n",
        "    def forward(self, x):\n",
        "        (z_quantized, dictionary_loss, commitment_loss, _) = self.quantize(x)\n",
        "        x_recon = self.decoder(z_quantized)\n",
        "        return {\n",
        "            \"dictionary_loss\": dictionary_loss,\n",
        "            \"commitment_loss\": commitment_loss,\n",
        "            \"x_recon\": x_recon,\n",
        "        }"
      ],
      "metadata": {
        "id": "Esv6mUD6Yv9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Training"
      ],
      "metadata": {
        "id": "aq2ReFOBHugn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "HPARAMS = dict(\n",
        "    num_channels   = 3,        # RGB\n",
        "    num_filters    = 128,      # base channel‐width\n",
        "    embedding_dim  = 64,\n",
        "    num_embeddings = 512,\n",
        "    use_ema        = True,\n",
        "    decay          = 0.99,\n",
        "    epsilon        = 1e-5,\n",
        ")\n",
        "\n",
        "model = VQVAE(**HPARAMS).to(DEVICE)\n",
        "\n",
        "# optimizer & loss\n",
        "LR         = 3e-4\n",
        "BETA       = 0.25                     # commitment-loss weight\n",
        "optimizer  = optim.Adam(model.parameters(), lr=LR)\n",
        "mse_loss   = nn.MSELoss()"
      ],
      "metadata": {
        "id": "v3Cmz0fGDSaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vqvae(model, loader, optimizer, mse_loss, beta, data_var,\n",
        "                epochs=7, eval_every=100, device=\"cuda\"):\n",
        "    model.train()\n",
        "    global_step = 0\n",
        "    best_loss   = float(\"inf\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss, running_recon = 0.0, 0.0\n",
        "        for batch_idx, (imgs, _) in enumerate(loader, 1):\n",
        "            imgs = imgs.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out  = model(imgs)\n",
        "\n",
        "            recon_err = mse_loss(out[\"x_recon\"], imgs) / data_var\n",
        "            loss      = recon_err + beta * out[\"commitment_loss\"]\n",
        "            if not model.vq.use_ema:\n",
        "                loss += out[\"dictionary_loss\"]\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss  += loss.item()\n",
        "            running_recon += recon_err.item()\n",
        "            global_step   += 1\n",
        "\n",
        "            if batch_idx % eval_every == 0:\n",
        "                mean_loss  = running_loss  / eval_every\n",
        "                mean_recon = running_recon / eval_every\n",
        "                best_loss  = min(best_loss, mean_loss)\n",
        "\n",
        "                print(f\"Epoch {epoch} | Step {global_step} \"\n",
        "                      f\"| loss {mean_loss:.4f}  (best {best_loss:.4f}) \"\n",
        "                      f\"| recon {mean_recon:.4f}\")\n",
        "\n",
        "                running_loss = running_recon = 0.0\n"
      ],
      "metadata": {
        "id": "VPLm30meI3ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS      = 7\n",
        "EVAL_EVERY  = 100\n",
        "\n",
        "train_vqvae(model,\n",
        "            train_loader,\n",
        "            optimizer,\n",
        "            mse_loss,\n",
        "            beta=BETA,\n",
        "            data_var=train_var,\n",
        "            epochs=EPOCHS,\n",
        "            eval_every=EVAL_EVERY,\n",
        "            device=DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gle1uPfcJofM",
        "outputId": "79e4c7ed-9737-4c40-b293-bfd792d8632b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Step 100 | loss 0.2719  (best 0.2719) | recon 0.2443\n",
            "Epoch 0 | Step 200 | loss 0.1851  (best 0.1851) | recon 0.1532\n",
            "Epoch 0 | Step 300 | loss 0.1743  (best 0.1743) | recon 0.1409\n",
            "Epoch 0 | Step 400 | loss 0.1646  (best 0.1646) | recon 0.1322\n",
            "Epoch 0 | Step 500 | loss 0.1588  (best 0.1588) | recon 0.1281\n",
            "Epoch 0 | Step 600 | loss 0.1533  (best 0.1533) | recon 0.1242\n",
            "Epoch 0 | Step 700 | loss 0.1449  (best 0.1449) | recon 0.1170\n",
            "Epoch 0 | Step 800 | loss 0.1465  (best 0.1449) | recon 0.1195\n",
            "Epoch 0 | Step 900 | loss 0.1417  (best 0.1417) | recon 0.1156\n",
            "Epoch 0 | Step 1000 | loss 0.1379  (best 0.1379) | recon 0.1130\n",
            "Epoch 0 | Step 1100 | loss 0.1354  (best 0.1354) | recon 0.1109\n",
            "Epoch 0 | Step 1200 | loss 0.1377  (best 0.1354) | recon 0.1138\n",
            "Epoch 0 | Step 1300 | loss 0.1333  (best 0.1333) | recon 0.1101\n",
            "Epoch 0 | Step 1400 | loss 0.1283  (best 0.1283) | recon 0.1059\n",
            "Epoch 0 | Step 1500 | loss 0.1281  (best 0.1281) | recon 0.1057\n",
            "Epoch 1 | Step 1663 | loss 0.1266  (best 0.1266) | recon 0.1055\n",
            "Epoch 1 | Step 1763 | loss 0.1231  (best 0.1231) | recon 0.1022\n",
            "Epoch 1 | Step 1863 | loss 0.1208  (best 0.1208) | recon 0.1006\n",
            "Epoch 1 | Step 1963 | loss 0.1212  (best 0.1208) | recon 0.1012\n",
            "Epoch 1 | Step 2063 | loss 0.1236  (best 0.1208) | recon 0.1040\n",
            "Epoch 1 | Step 2163 | loss 0.1190  (best 0.1190) | recon 0.1002\n",
            "Epoch 1 | Step 2263 | loss 0.1175  (best 0.1175) | recon 0.0990\n",
            "Epoch 1 | Step 2363 | loss 0.1160  (best 0.1160) | recon 0.0977\n",
            "Epoch 1 | Step 2463 | loss 0.1154  (best 0.1154) | recon 0.0970\n",
            "Epoch 1 | Step 2563 | loss 0.1150  (best 0.1150) | recon 0.0972\n",
            "Epoch 1 | Step 2663 | loss 0.1123  (best 0.1123) | recon 0.0944\n",
            "Epoch 1 | Step 2763 | loss 0.1123  (best 0.1123) | recon 0.0949\n",
            "Epoch 1 | Step 2863 | loss 0.1130  (best 0.1123) | recon 0.0955\n",
            "Epoch 1 | Step 2963 | loss 0.1093  (best 0.1093) | recon 0.0920\n",
            "Epoch 1 | Step 3063 | loss 0.1082  (best 0.1082) | recon 0.0914\n",
            "Epoch 2 | Step 3226 | loss 0.1062  (best 0.1062) | recon 0.0890\n",
            "Epoch 2 | Step 3326 | loss 0.1079  (best 0.1062) | recon 0.0915\n",
            "Epoch 2 | Step 3426 | loss 0.1074  (best 0.1062) | recon 0.0910\n",
            "Epoch 2 | Step 3526 | loss 0.1019  (best 0.1019) | recon 0.0863\n",
            "Epoch 2 | Step 3626 | loss 0.1060  (best 0.1019) | recon 0.0898\n",
            "Epoch 2 | Step 3726 | loss 0.1060  (best 0.1019) | recon 0.0900\n",
            "Epoch 2 | Step 3826 | loss 0.1078  (best 0.1019) | recon 0.0918\n",
            "Epoch 2 | Step 3926 | loss 0.1021  (best 0.1019) | recon 0.0869\n",
            "Epoch 2 | Step 4026 | loss 0.1033  (best 0.1019) | recon 0.0875\n",
            "Epoch 2 | Step 4126 | loss 0.1039  (best 0.1019) | recon 0.0886\n",
            "Epoch 2 | Step 4226 | loss 0.1007  (best 0.1007) | recon 0.0857\n",
            "Epoch 2 | Step 4326 | loss 0.1000  (best 0.1000) | recon 0.0851\n",
            "Epoch 2 | Step 4426 | loss 0.1033  (best 0.1000) | recon 0.0879\n",
            "Epoch 2 | Step 4526 | loss 0.1003  (best 0.1000) | recon 0.0851\n",
            "Epoch 2 | Step 4626 | loss 0.1014  (best 0.1000) | recon 0.0866\n",
            "Epoch 3 | Step 4789 | loss 0.0971  (best 0.0971) | recon 0.0826\n",
            "Epoch 3 | Step 4889 | loss 0.1009  (best 0.0971) | recon 0.0862\n",
            "Epoch 3 | Step 4989 | loss 0.0962  (best 0.0962) | recon 0.0816\n",
            "Epoch 3 | Step 5089 | loss 0.0959  (best 0.0959) | recon 0.0811\n",
            "Epoch 3 | Step 5189 | loss 0.0986  (best 0.0959) | recon 0.0841\n",
            "Epoch 3 | Step 5289 | loss 0.0992  (best 0.0959) | recon 0.0842\n",
            "Epoch 3 | Step 5389 | loss 0.0985  (best 0.0959) | recon 0.0840\n",
            "Epoch 3 | Step 5489 | loss 0.0928  (best 0.0928) | recon 0.0788\n",
            "Epoch 3 | Step 5589 | loss 0.0937  (best 0.0928) | recon 0.0797\n",
            "Epoch 3 | Step 5689 | loss 0.0943  (best 0.0928) | recon 0.0802\n",
            "Epoch 3 | Step 5789 | loss 0.0964  (best 0.0928) | recon 0.0819\n",
            "Epoch 3 | Step 5889 | loss 0.0947  (best 0.0928) | recon 0.0808\n",
            "Epoch 3 | Step 5989 | loss 0.0921  (best 0.0921) | recon 0.0785\n",
            "Epoch 3 | Step 6089 | loss 0.0940  (best 0.0921) | recon 0.0801\n",
            "Epoch 3 | Step 6189 | loss 0.0958  (best 0.0921) | recon 0.0817\n",
            "Epoch 4 | Step 6352 | loss 0.0936  (best 0.0921) | recon 0.0798\n",
            "Epoch 4 | Step 6452 | loss 0.0923  (best 0.0921) | recon 0.0786\n",
            "Epoch 4 | Step 6552 | loss 0.0930  (best 0.0921) | recon 0.0792\n",
            "Epoch 4 | Step 6652 | loss 0.0892  (best 0.0892) | recon 0.0757\n",
            "Epoch 4 | Step 6752 | loss 0.0909  (best 0.0892) | recon 0.0771\n",
            "Epoch 4 | Step 6852 | loss 0.0888  (best 0.0888) | recon 0.0754\n",
            "Epoch 4 | Step 6952 | loss 0.0953  (best 0.0888) | recon 0.0808\n",
            "Epoch 4 | Step 7052 | loss 0.0903  (best 0.0888) | recon 0.0768\n",
            "Epoch 4 | Step 7152 | loss 0.0882  (best 0.0882) | recon 0.0748\n",
            "Epoch 4 | Step 7252 | loss 0.0951  (best 0.0882) | recon 0.0810\n",
            "Epoch 4 | Step 7352 | loss 0.0927  (best 0.0882) | recon 0.0789\n",
            "Epoch 4 | Step 7452 | loss 0.0927  (best 0.0882) | recon 0.0791\n",
            "Epoch 4 | Step 7552 | loss 0.0904  (best 0.0882) | recon 0.0768\n",
            "Epoch 4 | Step 7652 | loss 0.0903  (best 0.0882) | recon 0.0766\n",
            "Epoch 4 | Step 7752 | loss 0.0894  (best 0.0882) | recon 0.0758\n",
            "Epoch 5 | Step 7915 | loss 0.0887  (best 0.0882) | recon 0.0753\n",
            "Epoch 5 | Step 8015 | loss 0.0895  (best 0.0882) | recon 0.0758\n",
            "Epoch 5 | Step 8115 | loss 0.0900  (best 0.0882) | recon 0.0763\n",
            "Epoch 5 | Step 8215 | loss 0.0897  (best 0.0882) | recon 0.0761\n",
            "Epoch 5 | Step 8315 | loss 0.0908  (best 0.0882) | recon 0.0769\n",
            "Epoch 5 | Step 8415 | loss 0.0898  (best 0.0882) | recon 0.0756\n",
            "Epoch 5 | Step 8515 | loss 0.0893  (best 0.0882) | recon 0.0757\n",
            "Epoch 5 | Step 8615 | loss 0.0886  (best 0.0882) | recon 0.0750\n",
            "Epoch 5 | Step 8715 | loss 0.0918  (best 0.0882) | recon 0.0776\n",
            "Epoch 5 | Step 8815 | loss 0.0919  (best 0.0882) | recon 0.0777\n",
            "Epoch 5 | Step 8915 | loss 0.0889  (best 0.0882) | recon 0.0752\n",
            "Epoch 5 | Step 9015 | loss 0.0887  (best 0.0882) | recon 0.0747\n",
            "Epoch 5 | Step 9115 | loss 0.0927  (best 0.0882) | recon 0.0785\n",
            "Epoch 5 | Step 9215 | loss 0.0875  (best 0.0875) | recon 0.0738\n",
            "Epoch 5 | Step 9315 | loss 0.0870  (best 0.0870) | recon 0.0733\n",
            "Epoch 6 | Step 9478 | loss 0.0873  (best 0.0870) | recon 0.0735\n",
            "Epoch 6 | Step 9578 | loss 0.0899  (best 0.0870) | recon 0.0758\n",
            "Epoch 6 | Step 9678 | loss 0.0901  (best 0.0870) | recon 0.0761\n",
            "Epoch 6 | Step 9778 | loss 0.0877  (best 0.0870) | recon 0.0740\n",
            "Epoch 6 | Step 9878 | loss 0.0886  (best 0.0870) | recon 0.0747\n",
            "Epoch 6 | Step 9978 | loss 0.0871  (best 0.0870) | recon 0.0732\n",
            "Epoch 6 | Step 10078 | loss 0.0885  (best 0.0870) | recon 0.0740\n",
            "Epoch 6 | Step 10178 | loss 0.0883  (best 0.0870) | recon 0.0742\n",
            "Epoch 6 | Step 10278 | loss 0.0865  (best 0.0865) | recon 0.0725\n",
            "Epoch 6 | Step 10378 | loss 0.0877  (best 0.0865) | recon 0.0737\n",
            "Epoch 6 | Step 10478 | loss 0.0873  (best 0.0865) | recon 0.0734\n",
            "Epoch 6 | Step 10578 | loss 0.0890  (best 0.0865) | recon 0.0748\n",
            "Epoch 6 | Step 10678 | loss 0.0860  (best 0.0860) | recon 0.0722\n",
            "Epoch 6 | Step 10778 | loss 0.0852  (best 0.0852) | recon 0.0714\n",
            "Epoch 6 | Step 10878 | loss 0.0864  (best 0.0852) | recon 0.0723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title loading the model I trained\n",
        "import gdown\n",
        "file_id = \"1zl7Ct4NUkSwVjgt0v666ag42EgkY6V1D\"\n",
        "output_path = \"vqvae_cifar10_pretrained.pt\"\n",
        "\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_path, quiet=False)\n",
        "\n",
        "HPARAMS = dict(\n",
        "    num_channels   = 3,\n",
        "    num_filters    = 128,\n",
        "    embedding_dim  = 64,\n",
        "    num_embeddings = 512,\n",
        "    use_ema        = True,\n",
        "    decay          = 0.99,\n",
        "    epsilon        = 1e-5,\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VQVAE(**HPARAMS).to(DEVICE)\n",
        "model.load_state_dict(torch.load(output_path, map_location=DEVICE))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "Jh3aYkuaZa3Z",
        "outputId": "f11cd434-bced-4509-e1f7-4e1262d4bca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1zl7Ct4NUkSwVjgt0v666ag42EgkY6V1D\n",
            "From (redirected): https://drive.google.com/uc?id=1zl7Ct4NUkSwVjgt0v666ag42EgkY6V1D&confirm=t&uuid=c9d1d929-140e-4e03-8d27-9cc293db2f22\n",
            "To: /content/vqvae_cifar10_pretrained.pt\n",
            "100%|██████████| 60.1M/60.1M [00:00<00:00, 105MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VQVAE(\n",
              "  (encoder): Encoder(\n",
              "    (encode): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.01)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.01)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.01)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_vq_conv): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (vq): VectorQuantizer(\n",
              "    (N_i_ts): SonnetExponentialMovingAverage()\n",
              "    (m_i_ts): SonnetExponentialMovingAverage()\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (decode): Sequential(\n",
              "      (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): Sequential(\n",
              "        (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.01)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.01)\n",
              "      )\n",
              "      (5): Conv2d(256, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (6): Sigmoid()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample_imgs, _ = next(iter(test_loader))\n",
        "    sample_imgs    = sample_imgs.to(DEVICE)\n",
        "    recon_imgs     = model(sample_imgs)[\"x_recon\"]\n",
        "\n",
        "save_img_grid(sample_imgs, nrows=4, filename=\"true\")\n",
        "save_img_grid(recon_imgs,  nrows=4, filename=\"recon\")\n",
        "print(\"Saved 'true.jpg' and 'recon.jpg'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMUolyQ5KBAh",
        "outputId": "50770840-30a0-4e9a-84aa-5c727bb14ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 'true.jpg' and 'recon.jpg'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original:\")\n",
        "display(IPImage(\"true.jpg\"))\n",
        "print(\"Reconstruction:\")\n",
        "display(IPImage(\"recon.jpg\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "lY2yaTrqWuLR",
        "outputId": "5382073d-fe40-45f3-cb95-191c11b96682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstruction:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title one image for clarity\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample_imgs, _ = next(iter(test_loader))\n",
        "    sample_imgs = sample_imgs.to(DEVICE)\n",
        "    recon_imgs  = model(sample_imgs)[\"x_recon\"]\n",
        "\n",
        "idx = 5\n",
        "input_img = sample_imgs[idx].cpu().permute(1, 2, 0).clamp(0, 1)\n",
        "recon_img = recon_imgs[idx].cpu().permute(1, 2, 0).clamp(0, 1)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(input_img)\n",
        "plt.title(\"Original\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(recon_img)\n",
        "plt.title(\"Reconstruction\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "cellView": "form",
        "id": "k7xZORjPeCfu",
        "outputId": "2805a550-f7e0-4ec3-eab9-879c7cd9d902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}