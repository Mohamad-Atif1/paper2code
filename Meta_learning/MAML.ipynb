{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOEvM4DmLmWA7iAUIDMd88j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamad-Atif1/paper2code/blob/main/Meta_learning/MAML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Meta learning background**\n",
        "\n",
        "Meta-learning, also known as ‚Äúlearning to learn,‚Äù is a subfield of machine learning that aims to develop models capable of quickly adapting to new tasks using only a small amount of data. The main idea is to train models that can generalize knowledge from previously seen tasks, enabling fast learning on unseen but related tasks.\n",
        "\n",
        "In typical supervised machine learning problems, we aim to create a model that performs well on a single task, so we train the model on that task and evaluate it on the same task. In contrast, meta-learning aims to train a model that can quickly adapt to many new tasks, so we train the model on a set of tasks and test it on entirely new, unseen tasks! Usually, you would only need to pass a few examples like 5 to 15.\n",
        "\n",
        "In the following sections, we will explore how to train such models by implementing [MAML (Model-Agnostic Meta-Learning)](https://arxiv.org/abs/1703.03400). But before that, let‚Äôs clarify some important terminology"
      ],
      "metadata": {
        "id": "xJuvw2044zUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Terminology**\n",
        "In supervised machine learning, we have a **dataset**  $ùíü =\\{(x,y)_k\\}$ which is a set of pairs of inputs and outputs (i.e., features and labels). We aim to minimize the loss which is typically the negative log likelihood $ \\operatorname{argmin} _Œ∏ ‚Ñí(Œ∏,ùíü)$\n",
        "\n",
        "In meta-learning, we have a multiple tasks that are sampled from a **task distribution** $p(ùõµ)$. Task is a dataset with its loss $ùõµ = \\{ùíü,‚Ñí\\}  $. Typically, each task is divided into a support set $ùíü^{tr}$ (used for adaptation) and a query set $ùíü^{test}$ (used for evaluation). In the few-shot setting, this is often described as N-way K-shot: each task is an N-class classification problem with K labeled examples per class in the support set, where both the features and their labels are used to adapt the model to the task . The query set contains additional labeled examples from those N classes; during evaluation, the model receives only the query features, and the labels are used to measure performance after adaptation. Formally, predictions are made as\n",
        "$$\n",
        "y^{ts} = f_Œ∏(ùíü^{tr},x^{ts})\n",
        "$$\n",
        "\n",
        "The goal is to find a learning algorithm $œï$ (sometimes called meta-learner or meta-parameters) that is used to find the optimal parameters $Œ∏^*$ on unseen tasks.\n",
        "\n",
        "All terms are described in the Figure below"
      ],
      "metadata": {
        "id": "iFTMBvfQFrtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/Mohamad-Atif1/paper2code/blob/main/Meta_learning/images/Terminology.png?raw=true\">"
      ],
      "metadata": {
        "id": "ag_B7Fu7D4-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This figure is taken from [Chelsea B. Finn dissertation](https://ai.stanford.edu/~cbfinn/_files/dissertation.pdf#page=13.14)*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WOtVo4ZrEnT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Agnostic Meta Learning"
      ],
      "metadata": {
        "id": "3QE6JwN2Fdaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model-Agnostic Meta-Learning (MAML) is an optimization-based method for few-shot learning. MAML focuses on the training process itself not on the model architecture; thats why it is called ‚Äúmodel-agnostic‚Äù because it does not depend on any specific model architecture, it works with any model that can be trained using gradient descent. This flexibility makes MAML broadly applicable to many types of problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "ARNomTzXPOLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAML algorithm"
      ],
      "metadata": {
        "id": "HQmqgZ_eZOKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: Initialize Meta-Parameters $œï$\n",
        "\n",
        "2: Randomly select a batch of tasks from the task distribution.\n",
        "\n",
        "3: For each sampled task ($Œ∏$ update):\n",
        "\n",
        "* Make a copy of the meta-parameters $œï$ (let‚Äôs call them $Œ∏$), initially $œï = Œ∏$\n",
        "\n",
        "\n",
        "* Use the support set $ùíü^{tr}$ from the current task to compute the loss with $Œ∏$\n",
        "\n",
        "* Perform a few steps of gradient descent on $Œ∏$ to get new parameters $Œ∏^*$.\n",
        "\n",
        "$$\n",
        "Œ∏^* = Œ∏ - Œ± ‚àá_Œ∏‚Ñí(ùíü^{tr},Œ∏)\n",
        "$$\n",
        "   \n",
        "*   The final $Œ∏$ is $Œ∏^*$\n",
        "\n",
        "\n",
        "4: Meta-Update ($\\phi$ Update):\n",
        "\n",
        "The loss function for meta is the average loss of all tasks using $Œ∏^*$\n",
        "\n",
        "$$\n",
        " ‚Ñí_{meta} = \\mathbb{E}_{ùõµ \\sim p(ùõµ)} \\left[\n",
        "    ‚Ñí_{ùõµ}\\left(Œ∏^*,ùíü^{test}\n",
        "    \\right)\n",
        "\\right\n",
        "]\n",
        "$$\n",
        "\n",
        "and we update\n",
        "\n",
        "$$\n",
        "œï = œï - Œ≤ ‚àá_œï ‚Ñí_{meta}(ùíü^{tr},ùíü^{test},œï)\n",
        "$$\n",
        "\n",
        "\n",
        "5: Go back to Step 2 and repeat for many iterations.\n",
        "\n",
        "\n",
        "Notice that a meta-update step requires  higher-order gradients (gradient through a gradient step) since we have two updates: $Œ∏^*$ and $œï$. This makes the computation quite expensive.\n",
        "One of the suggested solutions is to treat the inner update $Œ∏^*$ as a constant for the outer update $œï$ to avoid higher-order derivatives. This approach is known as FOMAML. For more details, see [First-Order MAML](https://arxiv.org/abs/1803.02999)\n",
        "\n",
        "\n",
        "\n",
        "In this notebook, we will apply the original MAML algorithm on omniglot dataset\n"
      ],
      "metadata": {
        "id": "dzBEXUZzcElF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TKDJZNLKJdeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding part\n"
      ],
      "metadata": {
        "id": "YNlJgtWyIjCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install higher"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-daQxRS4VoSr",
        "outputId": "40237f59-e222-4abe-f99a-9509eb57012d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed higher-0.2.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "wveJczujIp2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Omniglot dataset:\n",
        "\n",
        "\n",
        "it is a dataset of handwritten characters from many different alphabets (like Latin, Greek, Hindi, etc.).\n",
        "It was designed for few-shot learning ‚Äî each character has only 20 examples, written by different people.\n",
        "In total, it has 1,623 different characters from 50 alphabits"
      ],
      "metadata": {
        "id": "zzCEgnPHafxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.Lambda(lambda x: TF.rotate(x, random.choice([0, 90, 180, 270]))),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load the Omniglot dataset\n",
        "background_dataset = torchvision.datasets.Omniglot(\n",
        "    root='./', download=True,transform=train_transform, background=True\n",
        ")\n",
        "evaluation_dataset = torchvision.datasets.Omniglot(\n",
        "    root='./', download=True, transform=test_transform, background=False\n",
        ")\n",
        "\n",
        "print(f\"Number of background classes: {len(background_dataset._alphabets)}\")\n",
        "print(f\"Number of evaluation classes: {len(evaluation_dataset._alphabets)}\")\n",
        "print(f\"Number of images in background dataset: {len(background_dataset)}\")\n",
        "print(f\"Number of images in evaluation dataset: {len(evaluation_dataset)}\")\n"
      ],
      "metadata": {
        "id": "MzQqfKGrZPtx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84886af7-bd6d-4006-d714-2322877b4575"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.46M/9.46M [00:00<00:00, 416MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.46M/6.46M [00:00<00:00, 361MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of background classes: 30\n",
            "Number of evaluation classes: 20\n",
            "Number of images in background dataset: 19280\n",
            "Number of images in evaluation dataset: 13180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task sampler"
      ],
      "metadata": {
        "id": "5oJ6za2RMep2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "\n",
        "# class_label -> list of indices\n",
        "\n",
        "class_to_indices_train = defaultdict(list)\n",
        "for idx, (img, target) in enumerate(background_dataset):\n",
        "    class_to_indices_train[target].append(idx)\n",
        "\n",
        "class_to_indices_test = defaultdict(list)\n",
        "for idx, (img, target) in enumerate(evaluation_dataset):\n",
        "    class_to_indices_test[target].append(idx)\n",
        "\n",
        "def omniglot_task_sampler(dataset, n_way=5, k_shot=1, q_query=1,train=True):\n",
        "    \"\"\"\n",
        "    Samples a single N-way K-shot classification task from the Omniglot dataset.\n",
        "\n",
        "    For each task, this function randomly selects `n_way` classes and samples\n",
        "    `k_shot` support examples and `q_query` query examples per class. It returns\n",
        "    both support and query sets as image-label pairs.\n",
        "\n",
        "    The labels are remapped on each task, starting from 0 to (n_way - 1)\n",
        "    We do not use the original labels\n",
        "    \"\"\"\n",
        "\n",
        "    class_to_indices = class_to_indices_train if train else class_to_indices_test\n",
        "\n",
        "    # Sample N classes\n",
        "    classes = random.sample(list(class_to_indices.keys()), n_way)\n",
        "\n",
        "    support_indices, query_indices = [], []\n",
        "    for c in classes:\n",
        "        indices = random.sample(class_to_indices[c], k_shot + q_query)\n",
        "        support_indices += indices[:k_shot]\n",
        "        query_indices += indices[k_shot:]\n",
        "    support_images = torch.stack([dataset[i][0] for i in support_indices])\n",
        "    support_labels = torch.tensor([i // k_shot for i in range(len(support_indices))])\n",
        "    query_images = torch.stack([dataset[i][0] for i in query_indices])\n",
        "    query_labels = torch.tensor([i // q_query for i in range(len(query_indices))])\n",
        "\n",
        "\n",
        "\n",
        "    return (support_images, support_labels), (query_images, query_labels)\n"
      ],
      "metadata": {
        "id": "SV0vwp03Y7u7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oVCwjUCLb8RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "n_way = 5\n",
        "k_shot = 3\n",
        "q_query = 5\n",
        "\n",
        "(support_images, support_labels), (query_images, query_labels) = omniglot_task_sampler(background_dataset, n_way=n_way, k_shot=k_shot, q_query=q_query)\n",
        "\n",
        "# Display support images\n",
        "print(\"Support Images:\")\n",
        "num_support_images = len(support_images)\n",
        "cols = 10\n",
        "rows = math.ceil(num_support_images / cols)\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.5, rows * 1.5))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(num_support_images):\n",
        "    axes[i].imshow(support_images[i].squeeze(), cmap='gray')\n",
        "    axes[i].set_title(f'Label: {support_labels[i].item()}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "for j in range(num_support_images, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display query images\n",
        "print(\"\\nQuery Images:\")\n",
        "num_query_images = len(query_images)\n",
        "rows = math.ceil(num_query_images / cols)\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.5, rows * 1.5))\n",
        "axes = axes.flatten() # Flatten the axes array for easy iteration\n",
        "\n",
        "for i in range(num_query_images):\n",
        "    axes[i].imshow(query_images[i].squeeze(), cmap='gray')\n",
        "    axes[i].set_title(f'Label: {query_labels[i].item()}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "for j in range(num_query_images, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "Ph0GMzfKa91Z",
        "outputId": "204e6ee4-5328-4982-c3f7-9185849b3f27"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Images:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query Images:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x450 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "BQGggwpbNo0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class OmniglotConvNet(nn.Module):\n",
        "    def __init__(self, n_way):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.classifier = nn.Linear(64 * 1 * 1, n_way)\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "FnO_VnlwcEHm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAML Algorithm"
      ],
      "metadata": {
        "id": "v0cLvyDnNry8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As I mentioned, we will run gradients through the optimization steps ( gradient of the gradeint). But PyTorch does not track gradients through optimizer.step() by default. We will use the \"higher\" library that wraps the model and the optimizer into a functional version"
      ],
      "metadata": {
        "id": "I2odMFBYNycQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import higher\n",
        "\n",
        "def maml_step_full(\n",
        "    model,\n",
        "    train_dataset,\n",
        "    test_dataset,\n",
        "    n_way=5,\n",
        "    k_shot=1,\n",
        "    q_query=1,\n",
        "    inner_steps=1,\n",
        "    meta_batch_size=4,\n",
        "    inner_lr=0.4,\n",
        "    meta_lr=0.001,\n",
        "    eval_inner_lr=0.4,\n",
        "    eval_inner_steps=3,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "):\n",
        "    meta_optimizer = torch.optim.Adam(model.parameters(), lr=meta_lr)\n",
        "    model.train()\n",
        "    meta_optimizer.zero_grad()\n",
        "\n",
        "    meta_train_loss = 0.0\n",
        "    meta_train_acc = 0.0\n",
        "\n",
        "    for _ in range(meta_batch_size):\n",
        "        (support_images, support_labels), (query_images, query_labels) = omniglot_task_sampler(\n",
        "            train_dataset, n_way, k_shot, q_query\n",
        "        )\n",
        "        support_images, support_labels = support_images.to(device), support_labels.to(device)\n",
        "        query_images, query_labels = query_images.to(device), query_labels.to(device)\n",
        "\n",
        "\n",
        "        # This creates a differentiable copy of your model (fmodel)\n",
        "        # and a differentiable optimizer (diffopt) that tracks the update steps\n",
        "        # as part of the computation graph, allowing backpropagation\n",
        "        # and clean isolation of task-specific inner loops (copy_initial_weights=False)\n",
        "        # copy_initial_weights=False = for each task, take the orignal model\n",
        "        with higher.innerloop_ctx(model, torch.optim.SGD(model.parameters(), lr=inner_lr), copy_initial_weights=False) as (fmodel, diffopt):\n",
        "            for _ in range(inner_steps):\n",
        "                support_logits = fmodel(support_images)\n",
        "                support_loss = F.cross_entropy(support_logits, support_labels)\n",
        "                diffopt.step(support_loss)\n",
        "\n",
        "            query_logits = fmodel(query_images)\n",
        "            task_loss = F.cross_entropy(query_logits, query_labels)\n",
        "            meta_train_loss += task_loss\n",
        "\n",
        "            preds = torch.argmax(query_logits, dim=1)\n",
        "            acc = (preds == query_labels).float().mean().item()\n",
        "            meta_train_acc += acc\n",
        "\n",
        "    meta_train_loss /= meta_batch_size # Take the average of all q_tasks losses\n",
        "    meta_train_acc /= meta_batch_size\n",
        "    meta_train_loss.backward()\n",
        "    meta_optimizer.step()\n",
        "\n",
        "    # ========== Evaluation ========== #\n",
        "    meta_eval_loss = 0.0\n",
        "    meta_eval_acc = 0.0\n",
        "\n",
        "    for _ in range(meta_batch_size):\n",
        "        (support_images, support_labels), (query_images, query_labels) = omniglot_task_sampler(\n",
        "            test_dataset, n_way, k_shot, q_query,train=False\n",
        "        )\n",
        "        support_images, support_labels = support_images.to(device), support_labels.to(device)\n",
        "        query_images, query_labels = query_images.to(device), query_labels.to(device)\n",
        "\n",
        "        # No need for tracking high order gradient since we dont update the meta model\n",
        "        with higher.innerloop_ctx(model, torch.optim.SGD(model.parameters(), lr=eval_inner_lr), track_higher_grads=False,copy_initial_weights=False) as (fmodel, diffopt):\n",
        "            for _ in range(eval_inner_steps):\n",
        "                support_logits = fmodel(support_images)\n",
        "                support_loss = F.cross_entropy(support_logits, support_labels)\n",
        "                diffopt.step(support_loss)\n",
        "\n",
        "            query_logits = fmodel(query_images)\n",
        "            task_loss = F.cross_entropy(query_logits, query_labels)\n",
        "            meta_eval_loss += task_loss\n",
        "\n",
        "            preds = torch.argmax(query_logits, dim=1)\n",
        "            acc = (preds == query_labels).float().mean().item()\n",
        "            meta_eval_acc += acc\n",
        "\n",
        "    meta_eval_loss /= meta_batch_size\n",
        "    meta_eval_acc /= meta_batch_size\n",
        "\n",
        "    return {\n",
        "        \"train_loss\": meta_train_loss.item(),\n",
        "        \"train_acc\": meta_train_acc,\n",
        "        \"eval_loss\": meta_eval_loss.item(),\n",
        "        \"eval_acc\": meta_eval_acc,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "5SpCwc0fUm1H"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "num_steps = 60000\n",
        "model = OmniglotConvNet(n_way=5).to(device)\n",
        "for step in range(num_steps):\n",
        "    stats = maml_step_full(model, background_dataset, evaluation_dataset,q_query=15)\n",
        "    if step % 10 == 0:\n",
        "        print(f\"step: {step} [Train] Loss: {stats['train_loss']:.4f}, Acc: {stats['train_acc']:.4f} [Eval ] Loss: {stats['eval_loss']:.4f}, Acc: {stats['eval_acc']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orISj_00Q0Nf",
        "outputId": "0a60d31b-af29-4b33-cd6c-9ca749239478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0 [Train] Loss: 3.4960, Acc: 0.2000 [Eval ] Loss: 3.3805, Acc: 0.3233\n",
            "step: 10 [Train] Loss: 1.5187, Acc: 0.3600 [Eval ] Loss: 3.1602, Acc: 0.3700\n",
            "step: 20 [Train] Loss: 1.3973, Acc: 0.3800 [Eval ] Loss: 2.6275, Acc: 0.3267\n",
            "step: 30 [Train] Loss: 1.2732, Acc: 0.4600 [Eval ] Loss: 2.5291, Acc: 0.4800\n",
            "step: 40 [Train] Loss: 1.2482, Acc: 0.4433 [Eval ] Loss: 2.4840, Acc: 0.4233\n",
            "step: 50 [Train] Loss: 1.1734, Acc: 0.5100 [Eval ] Loss: 1.5497, Acc: 0.5600\n",
            "step: 60 [Train] Loss: 1.3518, Acc: 0.3667 [Eval ] Loss: 2.0548, Acc: 0.3933\n",
            "step: 70 [Train] Loss: 1.1688, Acc: 0.4833 [Eval ] Loss: 1.7559, Acc: 0.5133\n",
            "step: 80 [Train] Loss: 1.2718, Acc: 0.4200 [Eval ] Loss: 1.7724, Acc: 0.6267\n",
            "step: 90 [Train] Loss: 1.0719, Acc: 0.5633 [Eval ] Loss: 2.5083, Acc: 0.4867\n",
            "step: 100 [Train] Loss: 0.9095, Acc: 0.5533 [Eval ] Loss: 2.1955, Acc: 0.5167\n",
            "step: 110 [Train] Loss: 0.9973, Acc: 0.5200 [Eval ] Loss: 2.1605, Acc: 0.5300\n",
            "step: 120 [Train] Loss: 0.9314, Acc: 0.6267 [Eval ] Loss: 1.7382, Acc: 0.4667\n",
            "step: 130 [Train] Loss: 1.1344, Acc: 0.5067 [Eval ] Loss: 2.1557, Acc: 0.4567\n",
            "step: 140 [Train] Loss: 1.0210, Acc: 0.4733 [Eval ] Loss: 1.7209, Acc: 0.5133\n",
            "step: 150 [Train] Loss: 0.9036, Acc: 0.5833 [Eval ] Loss: 2.1218, Acc: 0.4633\n",
            "step: 160 [Train] Loss: 1.0665, Acc: 0.5700 [Eval ] Loss: 1.8510, Acc: 0.5000\n",
            "step: 170 [Train] Loss: 0.9583, Acc: 0.5800 [Eval ] Loss: 1.3328, Acc: 0.6333\n",
            "step: 180 [Train] Loss: 0.7961, Acc: 0.6633 [Eval ] Loss: 1.5647, Acc: 0.5700\n",
            "step: 190 [Train] Loss: 0.8551, Acc: 0.6433 [Eval ] Loss: 1.1788, Acc: 0.6533\n",
            "step: 200 [Train] Loss: 0.7758, Acc: 0.7000 [Eval ] Loss: 1.3636, Acc: 0.5933\n",
            "step: 210 [Train] Loss: 0.9293, Acc: 0.5700 [Eval ] Loss: 1.9125, Acc: 0.5100\n",
            "step: 220 [Train] Loss: 0.9211, Acc: 0.6567 [Eval ] Loss: 0.9880, Acc: 0.6733\n",
            "step: 230 [Train] Loss: 0.7635, Acc: 0.6933 [Eval ] Loss: 1.5139, Acc: 0.6200\n",
            "step: 240 [Train] Loss: 0.7578, Acc: 0.7133 [Eval ] Loss: 1.8143, Acc: 0.6100\n",
            "step: 250 [Train] Loss: 1.0407, Acc: 0.5700 [Eval ] Loss: 1.2781, Acc: 0.6800\n",
            "step: 260 [Train] Loss: 0.9015, Acc: 0.6200 [Eval ] Loss: 1.2496, Acc: 0.6367\n",
            "step: 270 [Train] Loss: 0.7884, Acc: 0.6900 [Eval ] Loss: 1.3929, Acc: 0.5667\n",
            "step: 280 [Train] Loss: 0.6433, Acc: 0.7700 [Eval ] Loss: 0.6475, Acc: 0.7600\n",
            "step: 290 [Train] Loss: 0.7127, Acc: 0.6933 [Eval ] Loss: 1.4318, Acc: 0.6533\n",
            "step: 300 [Train] Loss: 0.8014, Acc: 0.6667 [Eval ] Loss: 1.3511, Acc: 0.6367\n",
            "step: 310 [Train] Loss: 0.6626, Acc: 0.7133 [Eval ] Loss: 1.8319, Acc: 0.6067\n",
            "step: 320 [Train] Loss: 0.5998, Acc: 0.7933 [Eval ] Loss: 1.2687, Acc: 0.6233\n",
            "step: 330 [Train] Loss: 0.6081, Acc: 0.7367 [Eval ] Loss: 1.4741, Acc: 0.6133\n",
            "step: 340 [Train] Loss: 0.6936, Acc: 0.7533 [Eval ] Loss: 1.4047, Acc: 0.6033\n",
            "step: 350 [Train] Loss: 0.7603, Acc: 0.6767 [Eval ] Loss: 1.4237, Acc: 0.6200\n",
            "step: 360 [Train] Loss: 0.9817, Acc: 0.5267 [Eval ] Loss: 1.3521, Acc: 0.7267\n",
            "step: 370 [Train] Loss: 0.5587, Acc: 0.8067 [Eval ] Loss: 2.4665, Acc: 0.5333\n",
            "step: 380 [Train] Loss: 0.5495, Acc: 0.7833 [Eval ] Loss: 1.3387, Acc: 0.6967\n",
            "step: 390 [Train] Loss: 1.1399, Acc: 0.5833 [Eval ] Loss: 1.1152, Acc: 0.6500\n",
            "step: 400 [Train] Loss: 0.7260, Acc: 0.7200 [Eval ] Loss: 0.7885, Acc: 0.7133\n",
            "step: 410 [Train] Loss: 1.0082, Acc: 0.5867 [Eval ] Loss: 1.6184, Acc: 0.5900\n",
            "step: 420 [Train] Loss: 0.7123, Acc: 0.7067 [Eval ] Loss: 2.6931, Acc: 0.5167\n",
            "step: 430 [Train] Loss: 0.7214, Acc: 0.7300 [Eval ] Loss: 1.1441, Acc: 0.7067\n",
            "step: 440 [Train] Loss: 0.6453, Acc: 0.7333 [Eval ] Loss: 1.6382, Acc: 0.6133\n",
            "step: 450 [Train] Loss: 0.8950, Acc: 0.6500 [Eval ] Loss: 0.8062, Acc: 0.7900\n",
            "step: 460 [Train] Loss: 0.8638, Acc: 0.6667 [Eval ] Loss: 1.6864, Acc: 0.7133\n",
            "step: 470 [Train] Loss: 0.6728, Acc: 0.7033 [Eval ] Loss: 0.8973, Acc: 0.7167\n",
            "step: 480 [Train] Loss: 0.6124, Acc: 0.7367 [Eval ] Loss: 2.0372, Acc: 0.5800\n",
            "step: 490 [Train] Loss: 0.7627, Acc: 0.6533 [Eval ] Loss: 1.0318, Acc: 0.7367\n",
            "step: 500 [Train] Loss: 0.7603, Acc: 0.7100 [Eval ] Loss: 0.7916, Acc: 0.7900\n",
            "step: 510 [Train] Loss: 0.7148, Acc: 0.6567 [Eval ] Loss: 1.6123, Acc: 0.6367\n",
            "step: 520 [Train] Loss: 0.7414, Acc: 0.7167 [Eval ] Loss: 1.2974, Acc: 0.6733\n",
            "step: 530 [Train] Loss: 0.7619, Acc: 0.7267 [Eval ] Loss: 0.8732, Acc: 0.7200\n",
            "step: 540 [Train] Loss: 0.5970, Acc: 0.7533 [Eval ] Loss: 0.9161, Acc: 0.7633\n",
            "step: 550 [Train] Loss: 0.6185, Acc: 0.7933 [Eval ] Loss: 1.9238, Acc: 0.6567\n",
            "step: 560 [Train] Loss: 0.5948, Acc: 0.7567 [Eval ] Loss: 0.5680, Acc: 0.7900\n",
            "step: 570 [Train] Loss: 0.5236, Acc: 0.8333 [Eval ] Loss: 1.7536, Acc: 0.5933\n",
            "step: 580 [Train] Loss: 0.6229, Acc: 0.7667 [Eval ] Loss: 0.6720, Acc: 0.7933\n",
            "step: 590 [Train] Loss: 0.6066, Acc: 0.8100 [Eval ] Loss: 0.6625, Acc: 0.7533\n",
            "step: 600 [Train] Loss: 0.6474, Acc: 0.7267 [Eval ] Loss: 1.3680, Acc: 0.6067\n",
            "step: 610 [Train] Loss: 0.5755, Acc: 0.7933 [Eval ] Loss: 1.8753, Acc: 0.6300\n",
            "step: 620 [Train] Loss: 0.5635, Acc: 0.8167 [Eval ] Loss: 0.6192, Acc: 0.8100\n",
            "step: 630 [Train] Loss: 0.9335, Acc: 0.6333 [Eval ] Loss: 0.8431, Acc: 0.8133\n",
            "step: 640 [Train] Loss: 0.8701, Acc: 0.7000 [Eval ] Loss: 0.5650, Acc: 0.7767\n",
            "step: 650 [Train] Loss: 0.7799, Acc: 0.6767 [Eval ] Loss: 1.1830, Acc: 0.6867\n",
            "step: 660 [Train] Loss: 0.7263, Acc: 0.7333 [Eval ] Loss: 1.5616, Acc: 0.6067\n",
            "step: 670 [Train] Loss: 0.7907, Acc: 0.6633 [Eval ] Loss: 0.6130, Acc: 0.7900\n",
            "step: 680 [Train] Loss: 0.7299, Acc: 0.7267 [Eval ] Loss: 1.8255, Acc: 0.5800\n",
            "step: 690 [Train] Loss: 0.5595, Acc: 0.7700 [Eval ] Loss: 1.2637, Acc: 0.7233\n",
            "step: 700 [Train] Loss: 0.3810, Acc: 0.8733 [Eval ] Loss: 1.1185, Acc: 0.7333\n",
            "step: 710 [Train] Loss: 0.8001, Acc: 0.7267 [Eval ] Loss: 1.9270, Acc: 0.6500\n",
            "step: 720 [Train] Loss: 0.5878, Acc: 0.7567 [Eval ] Loss: 1.8767, Acc: 0.6133\n",
            "step: 730 [Train] Loss: 0.5997, Acc: 0.7400 [Eval ] Loss: 0.7141, Acc: 0.8033\n",
            "step: 740 [Train] Loss: 0.4049, Acc: 0.8367 [Eval ] Loss: 1.3403, Acc: 0.6267\n",
            "step: 750 [Train] Loss: 0.4868, Acc: 0.8167 [Eval ] Loss: 1.7910, Acc: 0.6667\n",
            "step: 760 [Train] Loss: 0.5802, Acc: 0.7700 [Eval ] Loss: 0.7820, Acc: 0.7000\n",
            "step: 770 [Train] Loss: 0.6221, Acc: 0.7367 [Eval ] Loss: 0.9406, Acc: 0.7100\n",
            "step: 780 [Train] Loss: 0.7193, Acc: 0.7000 [Eval ] Loss: 1.6452, Acc: 0.6400\n",
            "step: 790 [Train] Loss: 0.5985, Acc: 0.7733 [Eval ] Loss: 0.9109, Acc: 0.7567\n",
            "step: 800 [Train] Loss: 0.6565, Acc: 0.7433 [Eval ] Loss: 0.8850, Acc: 0.7133\n",
            "step: 810 [Train] Loss: 0.5746, Acc: 0.7733 [Eval ] Loss: 1.6031, Acc: 0.6467\n",
            "step: 820 [Train] Loss: 0.6095, Acc: 0.7133 [Eval ] Loss: 1.5241, Acc: 0.7300\n",
            "step: 830 [Train] Loss: 0.5936, Acc: 0.7600 [Eval ] Loss: 1.7426, Acc: 0.6900\n",
            "step: 840 [Train] Loss: 0.6005, Acc: 0.7700 [Eval ] Loss: 0.9916, Acc: 0.8267\n",
            "step: 850 [Train] Loss: 0.4522, Acc: 0.8567 [Eval ] Loss: 0.8206, Acc: 0.8000\n",
            "step: 860 [Train] Loss: 0.6786, Acc: 0.7100 [Eval ] Loss: 0.2690, Acc: 0.9133\n",
            "step: 870 [Train] Loss: 0.5233, Acc: 0.7933 [Eval ] Loss: 1.0715, Acc: 0.8100\n",
            "step: 880 [Train] Loss: 0.5752, Acc: 0.7867 [Eval ] Loss: 0.8119, Acc: 0.7367\n",
            "step: 890 [Train] Loss: 0.7743, Acc: 0.6633 [Eval ] Loss: 0.6116, Acc: 0.7933\n",
            "step: 900 [Train] Loss: 0.5612, Acc: 0.7767 [Eval ] Loss: 0.5057, Acc: 0.7967\n",
            "step: 910 [Train] Loss: 0.7156, Acc: 0.7500 [Eval ] Loss: 1.5046, Acc: 0.6600\n",
            "step: 920 [Train] Loss: 0.6769, Acc: 0.7100 [Eval ] Loss: 0.4385, Acc: 0.8533\n",
            "step: 930 [Train] Loss: 0.6235, Acc: 0.7500 [Eval ] Loss: 1.0169, Acc: 0.7333\n",
            "step: 940 [Train] Loss: 0.6580, Acc: 0.7433 [Eval ] Loss: 0.7695, Acc: 0.7967\n",
            "step: 950 [Train] Loss: 0.7396, Acc: 0.7067 [Eval ] Loss: 0.9126, Acc: 0.7833\n",
            "step: 960 [Train] Loss: 0.7358, Acc: 0.7267 [Eval ] Loss: 0.9185, Acc: 0.6867\n",
            "step: 970 [Train] Loss: 0.5960, Acc: 0.8000 [Eval ] Loss: 1.4282, Acc: 0.7100\n",
            "step: 980 [Train] Loss: 0.4254, Acc: 0.8533 [Eval ] Loss: 1.1472, Acc: 0.7733\n",
            "step: 990 [Train] Loss: 0.7216, Acc: 0.6933 [Eval ] Loss: 0.9557, Acc: 0.7100\n",
            "step: 1000 [Train] Loss: 0.6197, Acc: 0.7733 [Eval ] Loss: 0.9272, Acc: 0.6867\n",
            "step: 1010 [Train] Loss: 0.6608, Acc: 0.7733 [Eval ] Loss: 0.4378, Acc: 0.8367\n",
            "step: 1020 [Train] Loss: 0.8642, Acc: 0.6367 [Eval ] Loss: 0.5278, Acc: 0.8167\n",
            "step: 1030 [Train] Loss: 0.4423, Acc: 0.8633 [Eval ] Loss: 1.7104, Acc: 0.7000\n",
            "step: 1040 [Train] Loss: 0.3993, Acc: 0.8633 [Eval ] Loss: 1.3403, Acc: 0.6567\n",
            "step: 1050 [Train] Loss: 0.6614, Acc: 0.7367 [Eval ] Loss: 0.4746, Acc: 0.8200\n",
            "step: 1060 [Train] Loss: 0.4770, Acc: 0.8200 [Eval ] Loss: 0.6986, Acc: 0.8233\n",
            "step: 1070 [Train] Loss: 0.5197, Acc: 0.8033 [Eval ] Loss: 0.9459, Acc: 0.7233\n",
            "step: 1080 [Train] Loss: 0.5578, Acc: 0.7767 [Eval ] Loss: 0.5541, Acc: 0.8133\n",
            "step: 1090 [Train] Loss: 0.4993, Acc: 0.8000 [Eval ] Loss: 1.2074, Acc: 0.6933\n",
            "step: 1100 [Train] Loss: 0.4372, Acc: 0.8200 [Eval ] Loss: 0.6561, Acc: 0.8133\n",
            "step: 1110 [Train] Loss: 0.6708, Acc: 0.7267 [Eval ] Loss: 0.7237, Acc: 0.7733\n",
            "step: 1120 [Train] Loss: 0.5068, Acc: 0.8233 [Eval ] Loss: 0.7906, Acc: 0.7267\n",
            "step: 1130 [Train] Loss: 0.3693, Acc: 0.9133 [Eval ] Loss: 1.1668, Acc: 0.6400\n",
            "step: 1140 [Train] Loss: 0.7123, Acc: 0.7100 [Eval ] Loss: 1.5540, Acc: 0.6967\n",
            "step: 1150 [Train] Loss: 0.5398, Acc: 0.8033 [Eval ] Loss: 0.7743, Acc: 0.7600\n",
            "step: 1160 [Train] Loss: 0.7855, Acc: 0.7167 [Eval ] Loss: 1.6686, Acc: 0.6700\n",
            "step: 1170 [Train] Loss: 0.4506, Acc: 0.8667 [Eval ] Loss: 1.8188, Acc: 0.6933\n",
            "step: 1180 [Train] Loss: 0.4827, Acc: 0.8467 [Eval ] Loss: 0.7923, Acc: 0.8000\n",
            "step: 1190 [Train] Loss: 0.5997, Acc: 0.7833 [Eval ] Loss: 0.8116, Acc: 0.7567\n",
            "step: 1200 [Train] Loss: 0.5409, Acc: 0.8133 [Eval ] Loss: 1.1017, Acc: 0.7467\n",
            "step: 1210 [Train] Loss: 0.5478, Acc: 0.8333 [Eval ] Loss: 0.5698, Acc: 0.7967\n",
            "step: 1220 [Train] Loss: 0.5530, Acc: 0.8433 [Eval ] Loss: 0.8112, Acc: 0.7300\n",
            "step: 1230 [Train] Loss: 0.6842, Acc: 0.8033 [Eval ] Loss: 1.0508, Acc: 0.7433\n",
            "step: 1240 [Train] Loss: 0.3720, Acc: 0.8667 [Eval ] Loss: 0.8382, Acc: 0.7167\n",
            "step: 1250 [Train] Loss: 0.5744, Acc: 0.7633 [Eval ] Loss: 0.5945, Acc: 0.7867\n",
            "step: 1260 [Train] Loss: 0.8900, Acc: 0.6600 [Eval ] Loss: 0.8663, Acc: 0.7500\n",
            "step: 1270 [Train] Loss: 0.3985, Acc: 0.8567 [Eval ] Loss: 1.0892, Acc: 0.7033\n",
            "step: 1280 [Train] Loss: 0.5193, Acc: 0.8033 [Eval ] Loss: 0.5581, Acc: 0.8233\n",
            "step: 1290 [Train] Loss: 0.4809, Acc: 0.7933 [Eval ] Loss: 0.6136, Acc: 0.7933\n",
            "step: 1300 [Train] Loss: 0.4609, Acc: 0.8300 [Eval ] Loss: 0.7707, Acc: 0.8133\n",
            "step: 1310 [Train] Loss: 0.4947, Acc: 0.7867 [Eval ] Loss: 0.5258, Acc: 0.7900\n",
            "step: 1320 [Train] Loss: 0.3696, Acc: 0.8500 [Eval ] Loss: 0.7191, Acc: 0.8300\n",
            "step: 1330 [Train] Loss: 0.4933, Acc: 0.8000 [Eval ] Loss: 1.0460, Acc: 0.7133\n",
            "step: 1340 [Train] Loss: 0.4719, Acc: 0.8000 [Eval ] Loss: 1.4704, Acc: 0.5967\n",
            "step: 1350 [Train] Loss: 0.3824, Acc: 0.8433 [Eval ] Loss: 0.3938, Acc: 0.8667\n",
            "step: 1360 [Train] Loss: 0.5683, Acc: 0.7767 [Eval ] Loss: 1.0114, Acc: 0.7300\n",
            "step: 1370 [Train] Loss: 0.5689, Acc: 0.7733 [Eval ] Loss: 0.5001, Acc: 0.8433\n",
            "step: 1380 [Train] Loss: 0.4950, Acc: 0.7967 [Eval ] Loss: 1.0480, Acc: 0.6333\n",
            "step: 1390 [Train] Loss: 0.5115, Acc: 0.7933 [Eval ] Loss: 0.9987, Acc: 0.7633\n",
            "step: 1400 [Train] Loss: 0.4076, Acc: 0.8533 [Eval ] Loss: 1.4997, Acc: 0.7567\n",
            "step: 1410 [Train] Loss: 0.3867, Acc: 0.8667 [Eval ] Loss: 0.2815, Acc: 0.9167\n",
            "step: 1420 [Train] Loss: 0.4473, Acc: 0.8500 [Eval ] Loss: 0.3845, Acc: 0.8800\n",
            "step: 1430 [Train] Loss: 1.1256, Acc: 0.5933 [Eval ] Loss: 0.9158, Acc: 0.8300\n",
            "step: 1440 [Train] Loss: 0.5260, Acc: 0.8233 [Eval ] Loss: 1.2584, Acc: 0.7600\n",
            "step: 1450 [Train] Loss: 0.6310, Acc: 0.7433 [Eval ] Loss: 1.3916, Acc: 0.6967\n",
            "step: 1460 [Train] Loss: 0.3548, Acc: 0.8633 [Eval ] Loss: 0.7100, Acc: 0.7833\n",
            "step: 1470 [Train] Loss: 0.3718, Acc: 0.8800 [Eval ] Loss: 0.9202, Acc: 0.8000\n",
            "step: 1480 [Train] Loss: 0.5559, Acc: 0.8467 [Eval ] Loss: 1.2189, Acc: 0.7533\n",
            "step: 1490 [Train] Loss: 0.6227, Acc: 0.7800 [Eval ] Loss: 0.6329, Acc: 0.7667\n",
            "step: 1500 [Train] Loss: 0.4086, Acc: 0.8267 [Eval ] Loss: 0.7493, Acc: 0.7933\n",
            "step: 1510 [Train] Loss: 0.4169, Acc: 0.8633 [Eval ] Loss: 0.2736, Acc: 0.9133\n",
            "step: 1520 [Train] Loss: 0.4491, Acc: 0.8400 [Eval ] Loss: 0.7227, Acc: 0.7700\n",
            "step: 1530 [Train] Loss: 0.2880, Acc: 0.8767 [Eval ] Loss: 0.6543, Acc: 0.8433\n",
            "step: 1540 [Train] Loss: 0.4118, Acc: 0.8767 [Eval ] Loss: 0.6331, Acc: 0.8533\n",
            "step: 1550 [Train] Loss: 0.3190, Acc: 0.8967 [Eval ] Loss: 0.9901, Acc: 0.7467\n",
            "step: 1560 [Train] Loss: 0.6183, Acc: 0.7867 [Eval ] Loss: 1.2135, Acc: 0.6333\n",
            "step: 1570 [Train] Loss: 0.6113, Acc: 0.7667 [Eval ] Loss: 1.5033, Acc: 0.7067\n",
            "step: 1580 [Train] Loss: 0.3906, Acc: 0.8600 [Eval ] Loss: 0.3801, Acc: 0.8600\n",
            "step: 1590 [Train] Loss: 0.3332, Acc: 0.8867 [Eval ] Loss: 0.5932, Acc: 0.7767\n",
            "step: 1600 [Train] Loss: 0.4771, Acc: 0.8433 [Eval ] Loss: 0.4027, Acc: 0.8967\n",
            "step: 1610 [Train] Loss: 0.6120, Acc: 0.7367 [Eval ] Loss: 1.0400, Acc: 0.7800\n",
            "step: 1620 [Train] Loss: 0.3618, Acc: 0.8933 [Eval ] Loss: 0.4931, Acc: 0.8300\n",
            "step: 1630 [Train] Loss: 0.4208, Acc: 0.8400 [Eval ] Loss: 1.1196, Acc: 0.8067\n",
            "step: 1640 [Train] Loss: 0.4711, Acc: 0.8167 [Eval ] Loss: 0.6653, Acc: 0.8133\n",
            "step: 1650 [Train] Loss: 0.4216, Acc: 0.8267 [Eval ] Loss: 0.8399, Acc: 0.7267\n",
            "step: 1660 [Train] Loss: 0.4452, Acc: 0.8300 [Eval ] Loss: 0.5996, Acc: 0.8233\n",
            "step: 1670 [Train] Loss: 0.3882, Acc: 0.8933 [Eval ] Loss: 0.6190, Acc: 0.7800\n",
            "step: 1680 [Train] Loss: 0.4915, Acc: 0.7933 [Eval ] Loss: 0.8028, Acc: 0.7800\n",
            "step: 1690 [Train] Loss: 0.4808, Acc: 0.8167 [Eval ] Loss: 0.5307, Acc: 0.8033\n",
            "step: 1700 [Train] Loss: 0.5105, Acc: 0.7900 [Eval ] Loss: 0.8253, Acc: 0.7533\n",
            "step: 1710 [Train] Loss: 0.3662, Acc: 0.8633 [Eval ] Loss: 0.3959, Acc: 0.8300\n",
            "step: 1720 [Train] Loss: 0.4301, Acc: 0.8600 [Eval ] Loss: 0.3103, Acc: 0.9000\n",
            "step: 1730 [Train] Loss: 0.5714, Acc: 0.8433 [Eval ] Loss: 1.1960, Acc: 0.6867\n",
            "step: 1740 [Train] Loss: 0.3479, Acc: 0.8767 [Eval ] Loss: 1.1874, Acc: 0.6833\n",
            "step: 1750 [Train] Loss: 0.7672, Acc: 0.7067 [Eval ] Loss: 0.5079, Acc: 0.8433\n",
            "step: 1760 [Train] Loss: 0.6073, Acc: 0.7867 [Eval ] Loss: 1.2140, Acc: 0.6767\n",
            "step: 1770 [Train] Loss: 0.6287, Acc: 0.7700 [Eval ] Loss: 0.8019, Acc: 0.7667\n",
            "step: 1780 [Train] Loss: 0.4767, Acc: 0.8500 [Eval ] Loss: 0.5509, Acc: 0.8233\n",
            "step: 1790 [Train] Loss: 0.4278, Acc: 0.8167 [Eval ] Loss: 0.5405, Acc: 0.8133\n",
            "step: 1800 [Train] Loss: 0.4451, Acc: 0.8300 [Eval ] Loss: 0.2835, Acc: 0.9033\n",
            "step: 1810 [Train] Loss: 0.4809, Acc: 0.7933 [Eval ] Loss: 1.1410, Acc: 0.7933\n",
            "step: 1820 [Train] Loss: 0.6080, Acc: 0.7900 [Eval ] Loss: 1.0412, Acc: 0.8167\n",
            "step: 1830 [Train] Loss: 0.3785, Acc: 0.8567 [Eval ] Loss: 0.6488, Acc: 0.7933\n",
            "step: 1840 [Train] Loss: 0.5685, Acc: 0.7833 [Eval ] Loss: 0.7962, Acc: 0.8400\n",
            "step: 1850 [Train] Loss: 0.4960, Acc: 0.8167 [Eval ] Loss: 0.6940, Acc: 0.7633\n",
            "step: 1860 [Train] Loss: 0.6588, Acc: 0.7167 [Eval ] Loss: 0.7043, Acc: 0.7900\n",
            "step: 1870 [Train] Loss: 0.3280, Acc: 0.8567 [Eval ] Loss: 1.0868, Acc: 0.7400\n",
            "step: 1880 [Train] Loss: 0.4526, Acc: 0.8500 [Eval ] Loss: 0.3914, Acc: 0.8567\n",
            "step: 1890 [Train] Loss: 0.4259, Acc: 0.8200 [Eval ] Loss: 1.2641, Acc: 0.6867\n",
            "step: 1900 [Train] Loss: 0.3564, Acc: 0.8500 [Eval ] Loss: 0.8960, Acc: 0.8067\n",
            "step: 1910 [Train] Loss: 0.5782, Acc: 0.7933 [Eval ] Loss: 1.4973, Acc: 0.7300\n",
            "step: 1920 [Train] Loss: 0.2677, Acc: 0.9200 [Eval ] Loss: 1.1501, Acc: 0.7633\n",
            "step: 1930 [Train] Loss: 0.5605, Acc: 0.8433 [Eval ] Loss: 0.9959, Acc: 0.8267\n",
            "step: 1940 [Train] Loss: 0.4484, Acc: 0.8033 [Eval ] Loss: 0.4143, Acc: 0.8300\n",
            "step: 1950 [Train] Loss: 0.3449, Acc: 0.8967 [Eval ] Loss: 1.1500, Acc: 0.7500\n",
            "step: 1960 [Train] Loss: 0.3799, Acc: 0.8733 [Eval ] Loss: 0.6638, Acc: 0.8500\n",
            "step: 1970 [Train] Loss: 0.4397, Acc: 0.8433 [Eval ] Loss: 0.4254, Acc: 0.8700\n",
            "step: 1980 [Train] Loss: 0.6239, Acc: 0.7633 [Eval ] Loss: 0.3265, Acc: 0.9067\n",
            "step: 1990 [Train] Loss: 0.5274, Acc: 0.8467 [Eval ] Loss: 0.2326, Acc: 0.9167\n",
            "step: 2000 [Train] Loss: 0.4678, Acc: 0.8300 [Eval ] Loss: 0.7562, Acc: 0.8267\n",
            "step: 2010 [Train] Loss: 0.5392, Acc: 0.7833 [Eval ] Loss: 1.2527, Acc: 0.7600\n",
            "step: 2020 [Train] Loss: 0.3360, Acc: 0.8967 [Eval ] Loss: 1.4529, Acc: 0.7800\n",
            "step: 2030 [Train] Loss: 0.7270, Acc: 0.7167 [Eval ] Loss: 0.3818, Acc: 0.8567\n",
            "step: 2040 [Train] Loss: 0.4046, Acc: 0.8433 [Eval ] Loss: 1.1054, Acc: 0.8200\n",
            "step: 2050 [Train] Loss: 0.4064, Acc: 0.8467 [Eval ] Loss: 0.8811, Acc: 0.8333\n",
            "step: 2060 [Train] Loss: 0.3614, Acc: 0.8767 [Eval ] Loss: 1.6364, Acc: 0.8033\n",
            "step: 2070 [Train] Loss: 0.4153, Acc: 0.8500 [Eval ] Loss: 0.8553, Acc: 0.8267\n",
            "step: 2080 [Train] Loss: 0.3625, Acc: 0.8567 [Eval ] Loss: 1.2219, Acc: 0.6967\n",
            "step: 2090 [Train] Loss: 0.4430, Acc: 0.7733 [Eval ] Loss: 0.4842, Acc: 0.8500\n",
            "step: 2100 [Train] Loss: 0.4306, Acc: 0.8233 [Eval ] Loss: 0.6852, Acc: 0.8400\n",
            "step: 2110 [Train] Loss: 0.3822, Acc: 0.8733 [Eval ] Loss: 1.1490, Acc: 0.6267\n",
            "step: 2120 [Train] Loss: 0.3942, Acc: 0.8400 [Eval ] Loss: 1.2606, Acc: 0.6267\n",
            "step: 2130 [Train] Loss: 0.4084, Acc: 0.8433 [Eval ] Loss: 0.9979, Acc: 0.6833\n",
            "step: 2140 [Train] Loss: 0.5886, Acc: 0.7667 [Eval ] Loss: 1.4313, Acc: 0.7333\n",
            "step: 2150 [Train] Loss: 0.4897, Acc: 0.7867 [Eval ] Loss: 0.3811, Acc: 0.8867\n",
            "step: 2160 [Train] Loss: 0.5474, Acc: 0.7767 [Eval ] Loss: 0.3901, Acc: 0.8567\n",
            "step: 2170 [Train] Loss: 0.1967, Acc: 0.9667 [Eval ] Loss: 0.4251, Acc: 0.8933\n",
            "step: 2180 [Train] Loss: 0.6387, Acc: 0.7500 [Eval ] Loss: 0.6357, Acc: 0.8267\n",
            "step: 2190 [Train] Loss: 0.7713, Acc: 0.7733 [Eval ] Loss: 0.8075, Acc: 0.7867\n",
            "step: 2200 [Train] Loss: 0.3627, Acc: 0.8533 [Eval ] Loss: 0.7166, Acc: 0.7567\n",
            "step: 2210 [Train] Loss: 0.4183, Acc: 0.8100 [Eval ] Loss: 0.5262, Acc: 0.7967\n",
            "step: 2220 [Train] Loss: 0.4571, Acc: 0.8067 [Eval ] Loss: 0.3325, Acc: 0.8700\n",
            "step: 2230 [Train] Loss: 0.2369, Acc: 0.9067 [Eval ] Loss: 0.2302, Acc: 0.9133\n",
            "step: 2240 [Train] Loss: 0.4099, Acc: 0.8333 [Eval ] Loss: 0.2841, Acc: 0.8933\n",
            "step: 2250 [Train] Loss: 0.6010, Acc: 0.7567 [Eval ] Loss: 0.6762, Acc: 0.8300\n",
            "step: 2260 [Train] Loss: 0.4737, Acc: 0.8267 [Eval ] Loss: 0.5878, Acc: 0.8467\n",
            "step: 2270 [Train] Loss: 0.3596, Acc: 0.9000 [Eval ] Loss: 0.6351, Acc: 0.8400\n",
            "step: 2280 [Train] Loss: 0.4603, Acc: 0.8233 [Eval ] Loss: 0.7098, Acc: 0.8633\n",
            "step: 2290 [Train] Loss: 0.3521, Acc: 0.9033 [Eval ] Loss: 0.3647, Acc: 0.8667\n",
            "step: 2300 [Train] Loss: 0.2754, Acc: 0.9133 [Eval ] Loss: 0.6464, Acc: 0.8200\n",
            "step: 2310 [Train] Loss: 0.3262, Acc: 0.8800 [Eval ] Loss: 0.3566, Acc: 0.8900\n",
            "step: 2320 [Train] Loss: 0.4587, Acc: 0.7933 [Eval ] Loss: 0.4518, Acc: 0.8267\n",
            "step: 2330 [Train] Loss: 0.3021, Acc: 0.8700 [Eval ] Loss: 1.3209, Acc: 0.6900\n",
            "step: 2340 [Train] Loss: 0.3684, Acc: 0.8733 [Eval ] Loss: 1.3651, Acc: 0.7767\n",
            "step: 2350 [Train] Loss: 0.3272, Acc: 0.8633 [Eval ] Loss: 0.8765, Acc: 0.7833\n",
            "step: 2360 [Train] Loss: 0.4318, Acc: 0.7900 [Eval ] Loss: 0.7631, Acc: 0.8133\n",
            "step: 2370 [Train] Loss: 0.3635, Acc: 0.8567 [Eval ] Loss: 1.1695, Acc: 0.7067\n",
            "step: 2380 [Train] Loss: 0.2861, Acc: 0.9067 [Eval ] Loss: 0.5595, Acc: 0.7900\n",
            "step: 2390 [Train] Loss: 0.4732, Acc: 0.8067 [Eval ] Loss: 0.5456, Acc: 0.8400\n",
            "step: 2400 [Train] Loss: 0.5011, Acc: 0.8133 [Eval ] Loss: 1.1026, Acc: 0.7800\n",
            "step: 2410 [Train] Loss: 0.2773, Acc: 0.9167 [Eval ] Loss: 0.3544, Acc: 0.8867\n",
            "step: 2420 [Train] Loss: 0.4100, Acc: 0.8233 [Eval ] Loss: 0.5322, Acc: 0.8333\n",
            "step: 2430 [Train] Loss: 0.6557, Acc: 0.7367 [Eval ] Loss: 1.2964, Acc: 0.7500\n",
            "step: 2440 [Train] Loss: 0.3227, Acc: 0.8667 [Eval ] Loss: 1.1790, Acc: 0.7300\n",
            "step: 2450 [Train] Loss: 0.4940, Acc: 0.8200 [Eval ] Loss: 0.8802, Acc: 0.8167\n",
            "step: 2460 [Train] Loss: 0.4158, Acc: 0.8367 [Eval ] Loss: 1.2362, Acc: 0.7400\n",
            "step: 2470 [Train] Loss: 0.3517, Acc: 0.8367 [Eval ] Loss: 0.4211, Acc: 0.8533\n",
            "step: 2480 [Train] Loss: 0.3417, Acc: 0.8667 [Eval ] Loss: 1.7992, Acc: 0.6300\n",
            "step: 2490 [Train] Loss: 0.1740, Acc: 0.9700 [Eval ] Loss: 1.0515, Acc: 0.8333\n",
            "step: 2500 [Train] Loss: 0.5903, Acc: 0.7900 [Eval ] Loss: 0.4233, Acc: 0.8667\n",
            "step: 2510 [Train] Loss: 0.2828, Acc: 0.9067 [Eval ] Loss: 1.0517, Acc: 0.7867\n",
            "step: 2520 [Train] Loss: 0.4138, Acc: 0.8567 [Eval ] Loss: 0.8134, Acc: 0.8067\n",
            "step: 2530 [Train] Loss: 0.4560, Acc: 0.8100 [Eval ] Loss: 0.3815, Acc: 0.8867\n",
            "step: 2540 [Train] Loss: 0.2616, Acc: 0.8933 [Eval ] Loss: 0.3917, Acc: 0.8767\n",
            "step: 2550 [Train] Loss: 0.4664, Acc: 0.7967 [Eval ] Loss: 0.9887, Acc: 0.8433\n",
            "step: 2560 [Train] Loss: 0.3153, Acc: 0.8733 [Eval ] Loss: 0.5411, Acc: 0.8133\n",
            "step: 2570 [Train] Loss: 0.4287, Acc: 0.8133 [Eval ] Loss: 0.2340, Acc: 0.9200\n",
            "step: 2580 [Train] Loss: 0.2891, Acc: 0.9167 [Eval ] Loss: 0.4308, Acc: 0.8667\n",
            "step: 2590 [Train] Loss: 0.2676, Acc: 0.9200 [Eval ] Loss: 2.2533, Acc: 0.6900\n",
            "step: 2600 [Train] Loss: 0.3318, Acc: 0.9033 [Eval ] Loss: 1.3243, Acc: 0.7400\n",
            "step: 2610 [Train] Loss: 0.3584, Acc: 0.8300 [Eval ] Loss: 0.3349, Acc: 0.9000\n",
            "step: 2620 [Train] Loss: 0.3763, Acc: 0.8433 [Eval ] Loss: 1.1875, Acc: 0.8033\n",
            "step: 2630 [Train] Loss: 0.5108, Acc: 0.7667 [Eval ] Loss: 0.2545, Acc: 0.9167\n",
            "step: 2640 [Train] Loss: 0.3941, Acc: 0.8567 [Eval ] Loss: 0.4160, Acc: 0.8367\n",
            "step: 2650 [Train] Loss: 0.1583, Acc: 0.9567 [Eval ] Loss: 0.6764, Acc: 0.8267\n",
            "step: 2660 [Train] Loss: 0.3690, Acc: 0.8400 [Eval ] Loss: 0.4650, Acc: 0.8167\n",
            "step: 2670 [Train] Loss: 0.4016, Acc: 0.8433 [Eval ] Loss: 1.6849, Acc: 0.6867\n",
            "step: 2680 [Train] Loss: 0.2518, Acc: 0.9200 [Eval ] Loss: 0.4887, Acc: 0.8267\n",
            "step: 2690 [Train] Loss: 0.4481, Acc: 0.8033 [Eval ] Loss: 0.5920, Acc: 0.8333\n",
            "step: 2700 [Train] Loss: 0.3692, Acc: 0.8333 [Eval ] Loss: 0.6037, Acc: 0.8367\n",
            "step: 2710 [Train] Loss: 0.2917, Acc: 0.8967 [Eval ] Loss: 0.8845, Acc: 0.7767\n",
            "step: 2720 [Train] Loss: 0.5396, Acc: 0.7900 [Eval ] Loss: 0.7752, Acc: 0.7467\n",
            "step: 2730 [Train] Loss: 0.3757, Acc: 0.8600 [Eval ] Loss: 0.6199, Acc: 0.8000\n",
            "step: 2740 [Train] Loss: 0.5811, Acc: 0.7800 [Eval ] Loss: 1.2686, Acc: 0.7933\n",
            "step: 2750 [Train] Loss: 0.2281, Acc: 0.9367 [Eval ] Loss: 0.3755, Acc: 0.8667\n",
            "step: 2760 [Train] Loss: 0.5841, Acc: 0.8000 [Eval ] Loss: 0.3817, Acc: 0.8833\n",
            "step: 2770 [Train] Loss: 0.4693, Acc: 0.7967 [Eval ] Loss: 0.3486, Acc: 0.8500\n",
            "step: 2780 [Train] Loss: 0.5325, Acc: 0.8133 [Eval ] Loss: 0.3127, Acc: 0.8933\n",
            "step: 2790 [Train] Loss: 0.2814, Acc: 0.8967 [Eval ] Loss: 1.1964, Acc: 0.7767\n",
            "step: 2800 [Train] Loss: 0.3805, Acc: 0.8567 [Eval ] Loss: 0.9633, Acc: 0.8300\n",
            "step: 2810 [Train] Loss: 0.3567, Acc: 0.8600 [Eval ] Loss: 0.3297, Acc: 0.9000\n",
            "step: 2820 [Train] Loss: 0.3388, Acc: 0.8933 [Eval ] Loss: 0.2584, Acc: 0.9233\n",
            "step: 2830 [Train] Loss: 0.4172, Acc: 0.8267 [Eval ] Loss: 0.3401, Acc: 0.9033\n",
            "step: 2840 [Train] Loss: 0.3543, Acc: 0.8933 [Eval ] Loss: 0.8506, Acc: 0.8467\n",
            "step: 2850 [Train] Loss: 0.2800, Acc: 0.8900 [Eval ] Loss: 1.1938, Acc: 0.8133\n",
            "step: 2860 [Train] Loss: 0.4977, Acc: 0.7767 [Eval ] Loss: 0.9946, Acc: 0.8567\n",
            "step: 2870 [Train] Loss: 0.1605, Acc: 0.9667 [Eval ] Loss: 0.1717, Acc: 0.9533\n",
            "step: 2880 [Train] Loss: 0.2326, Acc: 0.9033 [Eval ] Loss: 0.5144, Acc: 0.8467\n",
            "step: 2890 [Train] Loss: 0.3657, Acc: 0.8733 [Eval ] Loss: 0.7888, Acc: 0.8033\n",
            "step: 2900 [Train] Loss: 0.3708, Acc: 0.8867 [Eval ] Loss: 0.2579, Acc: 0.9267\n",
            "step: 2910 [Train] Loss: 0.3042, Acc: 0.8800 [Eval ] Loss: 0.9920, Acc: 0.7867\n",
            "step: 2920 [Train] Loss: 0.5083, Acc: 0.7967 [Eval ] Loss: 0.6678, Acc: 0.8200\n",
            "step: 2930 [Train] Loss: 0.2020, Acc: 0.9300 [Eval ] Loss: 0.8088, Acc: 0.7933\n",
            "step: 2940 [Train] Loss: 0.4284, Acc: 0.8467 [Eval ] Loss: 0.8772, Acc: 0.8867\n",
            "step: 2950 [Train] Loss: 0.2711, Acc: 0.9167 [Eval ] Loss: 0.2995, Acc: 0.9200\n",
            "step: 2960 [Train] Loss: 0.2712, Acc: 0.9200 [Eval ] Loss: 1.5276, Acc: 0.6767\n",
            "step: 2970 [Train] Loss: 0.1718, Acc: 0.9400 [Eval ] Loss: 1.4190, Acc: 0.7100\n",
            "step: 2980 [Train] Loss: 0.4236, Acc: 0.8200 [Eval ] Loss: 1.9931, Acc: 0.6700\n",
            "step: 2990 [Train] Loss: 0.2734, Acc: 0.8967 [Eval ] Loss: 0.8853, Acc: 0.8600\n",
            "step: 3000 [Train] Loss: 0.3268, Acc: 0.8700 [Eval ] Loss: 1.7782, Acc: 0.7400\n",
            "step: 3010 [Train] Loss: 0.2939, Acc: 0.9200 [Eval ] Loss: 1.0307, Acc: 0.7867\n",
            "step: 3020 [Train] Loss: 0.3812, Acc: 0.8533 [Eval ] Loss: 1.8853, Acc: 0.6600\n",
            "step: 3030 [Train] Loss: 0.3131, Acc: 0.8700 [Eval ] Loss: 0.5304, Acc: 0.7900\n",
            "step: 3040 [Train] Loss: 0.3004, Acc: 0.9100 [Eval ] Loss: 1.1163, Acc: 0.8267\n",
            "step: 3050 [Train] Loss: 0.2558, Acc: 0.9167 [Eval ] Loss: 0.4610, Acc: 0.8500\n",
            "step: 3060 [Train] Loss: 0.3000, Acc: 0.8900 [Eval ] Loss: 0.4330, Acc: 0.8667\n",
            "step: 3070 [Train] Loss: 0.1850, Acc: 0.9433 [Eval ] Loss: 0.2117, Acc: 0.9400\n",
            "step: 3080 [Train] Loss: 0.2085, Acc: 0.9433 [Eval ] Loss: 0.6804, Acc: 0.8000\n",
            "step: 3090 [Train] Loss: 0.4840, Acc: 0.8033 [Eval ] Loss: 0.7596, Acc: 0.7733\n",
            "step: 3100 [Train] Loss: 0.2355, Acc: 0.9133 [Eval ] Loss: 0.7202, Acc: 0.7633\n",
            "step: 3110 [Train] Loss: 0.2125, Acc: 0.9200 [Eval ] Loss: 1.2575, Acc: 0.7233\n",
            "step: 3120 [Train] Loss: 0.2586, Acc: 0.9133 [Eval ] Loss: 0.5590, Acc: 0.7800\n",
            "step: 3130 [Train] Loss: 0.2111, Acc: 0.9367 [Eval ] Loss: 1.0979, Acc: 0.7533\n",
            "step: 3140 [Train] Loss: 0.4283, Acc: 0.8400 [Eval ] Loss: 0.5300, Acc: 0.8200\n",
            "step: 3150 [Train] Loss: 0.2113, Acc: 0.9367 [Eval ] Loss: 0.4328, Acc: 0.8400\n",
            "step: 3160 [Train] Loss: 0.3396, Acc: 0.8933 [Eval ] Loss: 1.6432, Acc: 0.6700\n",
            "step: 3170 [Train] Loss: 0.3175, Acc: 0.8633 [Eval ] Loss: 0.2748, Acc: 0.8933\n",
            "step: 3180 [Train] Loss: 0.4693, Acc: 0.8133 [Eval ] Loss: 0.9229, Acc: 0.7567\n",
            "step: 3190 [Train] Loss: 0.3503, Acc: 0.8433 [Eval ] Loss: 0.6052, Acc: 0.8167\n",
            "step: 3200 [Train] Loss: 0.3206, Acc: 0.8467 [Eval ] Loss: 0.5866, Acc: 0.8167\n",
            "step: 3210 [Train] Loss: 0.3150, Acc: 0.9100 [Eval ] Loss: 0.3458, Acc: 0.8933\n",
            "step: 3220 [Train] Loss: 0.2454, Acc: 0.9267 [Eval ] Loss: 0.8544, Acc: 0.8533\n",
            "step: 3230 [Train] Loss: 0.2443, Acc: 0.9133 [Eval ] Loss: 1.6426, Acc: 0.7133\n",
            "step: 3240 [Train] Loss: 0.3126, Acc: 0.8700 [Eval ] Loss: 0.9447, Acc: 0.8200\n",
            "step: 3250 [Train] Loss: 0.2174, Acc: 0.9133 [Eval ] Loss: 0.4415, Acc: 0.8467\n",
            "step: 3260 [Train] Loss: 0.2112, Acc: 0.9267 [Eval ] Loss: 0.6736, Acc: 0.8600\n",
            "step: 3270 [Train] Loss: 0.1556, Acc: 0.9533 [Eval ] Loss: 1.2189, Acc: 0.7333\n",
            "step: 3280 [Train] Loss: 0.4058, Acc: 0.8700 [Eval ] Loss: 0.3638, Acc: 0.8833\n",
            "step: 3290 [Train] Loss: 0.3244, Acc: 0.8833 [Eval ] Loss: 0.3643, Acc: 0.8667\n",
            "step: 3300 [Train] Loss: 0.2400, Acc: 0.9033 [Eval ] Loss: 0.9416, Acc: 0.7700\n",
            "step: 3310 [Train] Loss: 0.2779, Acc: 0.8633 [Eval ] Loss: 0.3932, Acc: 0.8533\n",
            "step: 3320 [Train] Loss: 0.1929, Acc: 0.9433 [Eval ] Loss: 0.9431, Acc: 0.8100\n",
            "step: 3330 [Train] Loss: 0.2944, Acc: 0.8733 [Eval ] Loss: 1.1157, Acc: 0.8400\n",
            "step: 3340 [Train] Loss: 0.3457, Acc: 0.9033 [Eval ] Loss: 1.0125, Acc: 0.8433\n",
            "step: 3350 [Train] Loss: 0.3994, Acc: 0.8533 [Eval ] Loss: 0.7235, Acc: 0.8133\n",
            "step: 3360 [Train] Loss: 0.3655, Acc: 0.8500 [Eval ] Loss: 1.1494, Acc: 0.7500\n",
            "step: 3370 [Train] Loss: 0.3683, Acc: 0.8633 [Eval ] Loss: 0.3361, Acc: 0.8900\n",
            "step: 3380 [Train] Loss: 0.5271, Acc: 0.7900 [Eval ] Loss: 0.6794, Acc: 0.9000\n",
            "step: 3390 [Train] Loss: 0.5655, Acc: 0.8033 [Eval ] Loss: 0.5281, Acc: 0.8067\n",
            "step: 3400 [Train] Loss: 0.3217, Acc: 0.8767 [Eval ] Loss: 0.5166, Acc: 0.8600\n",
            "step: 3410 [Train] Loss: 0.3922, Acc: 0.8400 [Eval ] Loss: 0.8344, Acc: 0.7833\n",
            "step: 3420 [Train] Loss: 0.4088, Acc: 0.8467 [Eval ] Loss: 0.8967, Acc: 0.8133\n",
            "step: 3430 [Train] Loss: 0.4726, Acc: 0.8467 [Eval ] Loss: 0.4060, Acc: 0.8800\n",
            "step: 3440 [Train] Loss: 0.2083, Acc: 0.9133 [Eval ] Loss: 0.6435, Acc: 0.8733\n",
            "step: 3450 [Train] Loss: 0.3897, Acc: 0.8667 [Eval ] Loss: 0.7005, Acc: 0.9167\n",
            "step: 3460 [Train] Loss: 0.3847, Acc: 0.8800 [Eval ] Loss: 0.3016, Acc: 0.9033\n",
            "step: 3470 [Train] Loss: 0.3445, Acc: 0.8533 [Eval ] Loss: 0.6898, Acc: 0.8667\n",
            "step: 3480 [Train] Loss: 0.4231, Acc: 0.8300 [Eval ] Loss: 1.6870, Acc: 0.7433\n",
            "step: 3490 [Train] Loss: 0.3830, Acc: 0.8567 [Eval ] Loss: 0.9571, Acc: 0.7633\n",
            "step: 3500 [Train] Loss: 0.2598, Acc: 0.9233 [Eval ] Loss: 0.2675, Acc: 0.9167\n",
            "step: 3510 [Train] Loss: 0.5369, Acc: 0.7933 [Eval ] Loss: 0.4957, Acc: 0.8733\n",
            "step: 3520 [Train] Loss: 0.5065, Acc: 0.7767 [Eval ] Loss: 0.3637, Acc: 0.8833\n",
            "step: 3530 [Train] Loss: 0.2298, Acc: 0.9000 [Eval ] Loss: 0.7348, Acc: 0.8200\n",
            "step: 3540 [Train] Loss: 0.4702, Acc: 0.8200 [Eval ] Loss: 1.2090, Acc: 0.7700\n",
            "step: 3550 [Train] Loss: 0.3381, Acc: 0.8633 [Eval ] Loss: 0.7637, Acc: 0.8467\n",
            "step: 3560 [Train] Loss: 0.2609, Acc: 0.9267 [Eval ] Loss: 1.5777, Acc: 0.7733\n",
            "step: 3570 [Train] Loss: 0.4052, Acc: 0.8633 [Eval ] Loss: 0.2333, Acc: 0.9333\n",
            "step: 3580 [Train] Loss: 0.2638, Acc: 0.9033 [Eval ] Loss: 0.1778, Acc: 0.9567\n",
            "step: 3590 [Train] Loss: 0.1372, Acc: 0.9633 [Eval ] Loss: 0.2543, Acc: 0.9167\n",
            "step: 3600 [Train] Loss: 0.2249, Acc: 0.8967 [Eval ] Loss: 0.2271, Acc: 0.9000\n",
            "step: 3610 [Train] Loss: 0.2556, Acc: 0.9233 [Eval ] Loss: 0.8149, Acc: 0.8033\n",
            "step: 3620 [Train] Loss: 0.2411, Acc: 0.9133 [Eval ] Loss: 0.3132, Acc: 0.8833\n",
            "step: 3630 [Train] Loss: 0.2311, Acc: 0.9233 [Eval ] Loss: 0.2867, Acc: 0.8833\n",
            "step: 3640 [Train] Loss: 0.2129, Acc: 0.9367 [Eval ] Loss: 0.4601, Acc: 0.8333\n",
            "step: 3650 [Train] Loss: 0.2638, Acc: 0.9167 [Eval ] Loss: 0.2348, Acc: 0.9300\n",
            "step: 3660 [Train] Loss: 0.4964, Acc: 0.8033 [Eval ] Loss: 0.9290, Acc: 0.8100\n",
            "step: 3670 [Train] Loss: 0.3345, Acc: 0.8633 [Eval ] Loss: 0.2802, Acc: 0.9100\n",
            "step: 3680 [Train] Loss: 0.2269, Acc: 0.9167 [Eval ] Loss: 0.7157, Acc: 0.8633\n",
            "step: 3690 [Train] Loss: 0.2508, Acc: 0.8867 [Eval ] Loss: 0.2963, Acc: 0.9100\n",
            "step: 3700 [Train] Loss: 0.2314, Acc: 0.9300 [Eval ] Loss: 0.6992, Acc: 0.8400\n",
            "step: 3710 [Train] Loss: 0.3498, Acc: 0.8800 [Eval ] Loss: 0.4411, Acc: 0.8900\n",
            "step: 3720 [Train] Loss: 0.2831, Acc: 0.8833 [Eval ] Loss: 1.0536, Acc: 0.8200\n",
            "step: 3730 [Train] Loss: 0.3075, Acc: 0.9067 [Eval ] Loss: 0.7293, Acc: 0.8567\n",
            "step: 3740 [Train] Loss: 0.2368, Acc: 0.9267 [Eval ] Loss: 1.0683, Acc: 0.7600\n",
            "step: 3750 [Train] Loss: 0.4309, Acc: 0.8233 [Eval ] Loss: 0.9425, Acc: 0.7400\n",
            "step: 3760 [Train] Loss: 0.5645, Acc: 0.7800 [Eval ] Loss: 0.6507, Acc: 0.8200\n",
            "step: 3770 [Train] Loss: 0.2329, Acc: 0.9233 [Eval ] Loss: 0.3007, Acc: 0.9033\n",
            "step: 3780 [Train] Loss: 0.4661, Acc: 0.8000 [Eval ] Loss: 0.2930, Acc: 0.9167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "IS45vAhVehMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MAML paper](https://arxiv.org/abs/1703.03400)\n",
        "\n",
        "[Chelsea B. Finn dissertation (Learning to Learn with Gradients)](https://ai.stanford.edu/~cbfinn/_files/dissertation.pdf#page=13.14)\n",
        "\n",
        "[Meta-Learning in Neural Networks: A Survey\n",
        "](https://arxiv.org/abs/2004.05439)\n",
        "\n"
      ],
      "metadata": {
        "id": "QsvEvCH3ejXr"
      }
    }
  ]
}